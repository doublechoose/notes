## 微调神经网络的超参数

神经网络有很多超参数要调整。您可以使用任何可以想象的网络拓扑结构
（神经元如何相互关联），甚至在一个简单的MLP中，你可以改变层的数量，每层的神经元数，激活函数要用哪个，权重初始化的逻辑，等等。你怎么知道什么样的超参数组合最适合你的任务呢？

当然，你可以用交叉验证的格搜索（grid search）来找到证券的超参数，但有如此多的超参数要调整，并且在一个大数据集上训练神经网络需要很多时间，在一定时间内，你只能探索到超参数空间的一小部分。最好是用[randomized search](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf).另一种选择是使用工具如[Oscar](http://oscar.calldesk.ai/)，它实现了更复杂的算法帮助你快速找到好的超参数组合。

了解每个超参数的合理值范围是多少有助于限定搜索范围。

### 隐藏层数量

对于许多问题，你可以只用一个隐藏层就能得到一个合理的结果。事实上，已经证明，只有一个隐藏层的MLP可以对最复杂的功能进行建模，只要它有足够的神经元。很长时间里，这些事实让研究员相信不需要再研究任何更深层的神经网络。但是深层网络比浅层具有更高的参数效率：他们可以使用比浅网络更少的神经元模拟复杂的函数，让他们训练更快。

假设让你用一些绘画软件画一个森林，但不能用复制粘贴。你就只能独立的画每棵树，每一个树枝，每一片叶子。如果你可以复制粘贴叶子到树枝上，然后复制粘贴树枝来创建一棵树，最后复制粘贴这棵树来创建森林，这很快就能完成。现实中，数据往往是由这种层级方式构成的，并且DNN自动利用这个事实：较低的隐藏层模拟低层的结构（线，各种形状的片段和方向），中间的隐藏层连接这些低级结构模拟中级结构（正方型，圆），然后最高层的隐藏层和输出层连接这些中级结构成更高级的结构（面）

这种层级架构不仅可以帮助DNN更快收敛到一个更好的解决方案，也提高了推广到新数据集的能力。比如，你已经训练了一个模型来识别照片上的人脸，现在你想训练一个新的神经网络来识别发型，那么你可以复用第一个网络的低层神经层，不再随机的生成权重值和bias，而是使用第一网络里的权重和bias初始化之。这样网络就不必从头开始训练那些在大部分照片存在的所有低级结构，它只要去学高级别的结构（发型）就可以了。

综上，对于许多问题，你可以开始使用一到两个隐藏层，它就可以做的不错（一个带百来个神经元的隐藏层可以在MNIST中达到97%的准确率，使用相同数量的神经元的2层隐藏层则可以达到98%）。对于更复杂的问题，你可以增加隐藏层的数量，直到开始过度拟合。非常复杂的任务，如大图片分类或者语音识别，通常需要几打层（甚至百来个，但不是全连接），并且需要大量的训练数据。然而你可以不用从头开始训练：通常重用执行类似任务的预训练部分。训练就会更快，并且需要的数据量更少。

### 每个隐藏层的神经元数量

显然，输入和输出层的神经元数量取决于输入的类型和任务输出的要求。比如MNIST任务需要28x28=784的输入神经和10个输出神经元，对于隐藏层，常用的是将他们的大小形成一个漏斗。每层的神经元越来越少---- 许多低级特征可以结合成更少的高级特征。比如一个典型的MNIST的神经网络有2个隐藏层，第一个有300个神经元第二个有100个神经元。然而该实践不再常见，你可以对所有隐藏层都设置相同数量的神经元----比如每个隐藏层有150个神经元：这样变成只要调整一个超参数而不是每层调整一个。就像层数，你可以试着逐渐增加神经元的数量直到神经网络开始过度拟合。一般，你可以通过增加层数来获得更多的效果。不过正如你所看到的，找到神经元的完美数量就像变魔术一样。

一个简单的操作是选择一个比你实际需要更多层和更多神经元的模型，然后尽早停止来防止过度拟合（和其他技术，主要是**dropout**）。这被叫做“弹力裤”的方法：不是花时间找适合你穿的裤子，而是直接拿一个特大裤子，然后收缩到正确的尺寸。

### 激活函数

大多数情况下，你可以在隐藏层中使用ReLU激活函数（或者它的变体），这比其他激活函数计算来的快，并且梯度下降不会卡在高原上，由于它不会饱和大输入值（logistic函数或者hyperbolic tangent  函数会在1处饱和）

对于输出层，softmax激活函数对于分类任务而言，是个好选择（当类别是相互排斥的时候）。对于回归任务，你可以不用任何激活函数。

