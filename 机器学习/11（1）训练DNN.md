

之前提到的，都是非常浅的DNN，只有2个隐藏层，如果你需要处理一个非常复杂的问题时，如检查数百种类型的高分辨率图像中的对象？你就需要训练一个更深点的DNN，可能10层，每层包含数百个神经元，有成千上百条连接。这就不像在公园散步那么容易了：

- 首先，你面临着梯度消失问题（或者梯度爆炸问题）影响深度神经网络并且让低层难以训练。
- 第二，这么大的神经网络，训练会特别的慢。
- 第三，一个具有百万个参数的模型可能会严重过度拟合训练集

接下来要讲下这几个问题和提出解决这些问题的技术。我们将解释梯度消失问题和几个解决此问题常用到的解决方案。接下来我们将看下能加快大型模型的训练速度的各种优化器。最后介绍一些修行的正规化大型神经网络技术。

## 梯度消失/爆炸问题

反向传播算法从输出层到输入层传播错误梯度。当算法为网络的每个参数计算损失函数的梯度，它根据梯度下降法使用这些梯度来更新每个参数。

当算法到更低神经层时，梯度也会变得越来越小。于是，梯度下降更新没改变多少低层的连接权重，训练不能收敛到一个好的解决方案中。这就是**梯度消失**问题。有些情况下，相反情况会发生：梯度下降会变得越来越大，许多层会更新特别大的权重，算法偏离。这叫**梯度爆炸**，这在RNN中经常发生。简单的说，深度神经网络遭受不稳定的梯度，不同层次可能以不同速度学习。

这是为什么深度神经网络在很长的一段时间里被弃用的原因之一。

直到2010年才取得重大进展。Xavier Glorot 和 Yoshua Bengio 发现一些问题，包括当时流行的logistic sigmoid激活函数和权重初始化技术，使用一个均值为0，标准差为1的正态分布随机初始化权重。简而言之，他们表示使用这个激活函数和这个初始化方案的结合，每层输出的方差远大于其输入的方差。在网络前行时，方差每经过一层就变大直到激活函数在顶层达到饱和。因为logistic函数的均值是0.5而让这情况变得更糟（hyperbolic tangent 函数均值是0，这让它在DNN工作的比logistic好）

看下，logistic函数，你可以看到当输入变大（正或负），函数饱和在0或1时,导数趋向于0。因此当反向传播进来后，实际上没有梯度传播进网络，当小梯度继续传播，梯度变得稀疏，到低层就没剩多少东西了。

### Xavier 和他的初始化 

在他们的论文中，Glorot 和 Bengio 提出了一种方法显著的减缓这个问题。我们需要信号在2个方向适当的流动：做预测的向前方向和反向传播梯度的方向。我们不希望信号中断，我们也不想它爆炸或者饱和。为了让信号适当的流动，作者讨论出，我们需要让每层输出的方差等于他们输入的方差，我们也需要这个梯度在流过层中之前和之后（反向）具有相等的方差（如果你对数学感兴趣，请查看论文
细节）。实际上没法保证2个都相等，除非层有相同的输入和输出连接，但是他们提出了一个折中的方案，在实践中十分实用：连接权重必须按照下面的随机初始化，其中ninputs 和 noutputs是要初始化权重的层的输入和输出连接。这个初始化策略被叫做Xavier初始化，或者Glorot初始化。
![1530164381.png](https://upload-images.jianshu.io/upload_images/3509189-da66e60fae72937d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



当输入连接等于输出连接时，可以得到一个简单等式

![1530164537.png](https://upload-images.jianshu.io/upload_images/3509189-d11542c0294320c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


使用Xavier初始化策略可以加速训练，并且可以让当前的深度学习成功的一个撇步。最近一些论文提供了类似的策略对不同的激活函数，如下。

![1530164744.png](https://upload-images.jianshu.io/upload_images/3509189-993b1c7d7b59906f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

ReLU激活函数（还有它的变种ELU）的初始化策略叫He初始化。

fully_connected()方法默认使用Xavier初始化，可以通过使用vafiance_scaling_initializer()方法改为He初始化：

```python
he_init = tf.contrib.layers.variance_scaling_initialzer()
hidden1 = fully_connected(X, n_hidden1,weights_initializer=he_init,scope="h1")
```

### 非饱和激活函数

Glorot和Bengio在2010年的见解是梯度下降\爆炸的问题有一部分原因是激活函数使用不当。那时许多人认为，既然大自然选择在生物神经元中使用sigmoid激活函数，那么sigmoid肯定是最好的选择。但事实打脸，其他激活函数在深度神经网络中表现的更好，特别是ReLU激活函数，主要是因为他对正值不饱和（也因为计算快）。

然而ReLU激活函数仍不是完美的。它碰到一个问题叫濒死的ReLU：训练时，一些神经元快速的死亡，这意味他们只输出0。有时候你可能会发现你网络的神经元死了一半，特别是当你用一个很大的学习率的时候。如果一个神经元的权重更新如权重和为负数时，它就会开始输出0。这时神经元就不能复活了，因为ReLU函数是当输入负时为0。

为了解决这个问题，你可能会想用ReLU函数的变体，如 **会漏的 ReLU**。这个函数定义为LeakyReLUα(z) = max(αz, z)  。超参数a定义函数“漏”多少：是当z<0时，函数的斜率，通常设置为0.01.这个小斜率保证**会漏的ReLU**永远不会死；他们可能会陷入昏迷，但有机会醒来。实际上设置a = 0.2（大漏）似乎比a=0.01（小漏）来的好。他们还评估了**随机漏的ReLU（RReLU）**，这里a在训练时随机的在一个范围里取值，在测试过程中设置为平均值。它也表现相当不错，并且似乎可以作正规者（减少过渡拟合的风险），最后，还有一种叫**参数化漏的ReLU（PReLU）**，a在训练中学习（不是作为超参数，而是变成一个参数，能被反向传播修改），这在大图片数据集表现的比ReLU好，但在小数据集中会提高过度拟合的风险。

最后，Djork-Arné Clevert 等人在2015年论文中提出一种新的激活函数，叫**指数线性单元（ELU）**，比任何ReLU变体都来的好：训练时间降低，神经网络在测试集上形成得更好。


![1530167106.png](https://upload-images.jianshu.io/upload_images/3509189-3892d88b3fc2fb2f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

有点像ReLU函数，但是有几点不同：

- 当z<0时取值为负，这可以让单元有一个平均接近于0的输出。这帮助减缓梯度消失问题，超参数a定义了当z是一个大负数时ReLU接近的值，通常设定为1，但你可以修改。
- 第二，当z<0,它有一个非零梯度，这避免了单元死亡问题。
- 第三，函数处处都是平滑的，包括z=0的时候，这可以加速梯度下降，因为在z=0的时候，左边和右边不会跳起（左右导数相等）

ELU的主要缺点是计算比ReLU和它的变体来的慢（因为用到了指数函数）。但在训练中这由更快的收敛弥补了。然而在测试时间，ELU网络还是会比ReLU网络来的慢。

那么，你要用哪种激活函数呢？通常来说：

ELU>会漏的ReLU(和它的变体) > ReLU > tanh > logistic

如果你关注运行性能，你最好用会漏的ReLU。如果你不想修改其他超参数，只要用之前提的默认的a（会漏的ReLU是0.01，ELU是1）。如果你有空闲时间和算力，你可以用交叉验证来评估其他激活函数，如果你的网络过渡拟合了可以用RReLU，或者你的数据集很大可以用PReLU。

tf提供了一个方法elu()可以拿来构建你的神经网络。只要设置activation_fn：

```python
hidden1 = fully_connected(X,n_hidden1,activation_fn=tf.nn.elu)
```

tf没有提供会漏的ReLU，但很容易定义：

```
def leaky_relu(z,name=None):
	return tf.maximum(0.01*z,z,name=name)

hidden1 = fully_connected(X,n_hidden1,activation_fn=leaky_relu)
```



反向传播：https://www.cnblogs.com/charlotte77/p/5629865.html
