## 复用预训练层

从0开始训练一个非常大的DNN是一个笨方法：你应该试着寻找已有的完成类似任务的神经网络，然后复用这个网络的较低层：这叫**迁移学习（transfer learning）**。这不仅加速训练，而且只需要较少训练数据。

比如，假设你有一个已经训练好能分类100种类别包括动物植物，车辆和日常用品的DNN。现在，你想训练一个DNN来分类特定的汽车。这些任务十分相似，所以你应该试着复用第一个网络的部分


![1530179071.png](https://upload-images.jianshu.io/upload_images/3509189-bde80ab8c8b3561d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果新任务的输入图片与原始任务中使用的尺寸大小不一致，则必须添加预处理步骤来将大小调整为原始模型的尺寸。简单说就是，转移学习只能在输入具有相同级别的特征才能很好的工作。

### 复用一个tf模型

如果原始模型使用tf训练的，你可以简单的恢复它并在新的任务中训练：

```python
[...] #构建原始模型

with tf.Session as sess:
    saver.restore(sess,"./my_original_model.ckpt")
    [...] # 在你的新任务中训练
    
```

然而，一般来说，你只想复用原始模型中的一部分。一个简单的解决是配置Saver只恢复原始模型中变量的子集。比如下面代码只恢复隐藏层1,2,3：

```python
[...] # 构建一个新模型

init = tf.global_variables_initializer()

reuse_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
scope="hidden[123]")
reuse_vars_dict = dict([(var.name, var.name) for var in reuse_vars])
original_saver = tf.Saver(reuse_vars_dict) # saver to restore the original model
new_saver = tf.Saver() # saver to save the new model
with tf.Session() as sess:
	sess.run(init)
	original_saver.restore("./my_original_model.ckpt") # restore layers 1 to 3
	[...] # train the new model
	new_saver.save("./my_new_model.ckpt") # save the whole model
```
首先我们构建新模型，拷贝原始模型隐藏层123.然后我们创建一个节点初始化所有变量。然后我们得到"trainable=True"（默认）的变量列表，下一步，创建一个字典映射原模型的变量名到新模型上（名字通常一致），然后创建Saver恢复这些变量，另一个Saver保存整个模型。最后在新任务上训练模型并保存。

任务越相近，可复用层数更多。对于非常相似的任务，完全可以保留所有的隐藏层，只要修改输出层就好了。

### 复用其他框架的模型

如果模型是由其他框架训练的，你要手动的导入权重（用Theano训练的，用Theano代码导入）。然后分配到适当的变量，这很乏味。

```
original_w = [...] # Load the weights from the other framework
original_b = [...] # Load the biases from the other framework

X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
hidden1 = fully_connected(X, n_hidden1, scope="hidden1")
[...] # # Build the rest of the model

# Get a handle on the variables created by fully_connected()
with tf.variable_scope("", default_name="", reuse=True): # root scope
	hidden1_weights = tf.get_variable("hidden1/weights")
	hidden1_biases = tf.get_variable("hidden1/biases")

# Create nodes to assign arbitrary values to the weights and biases
original_weights = tf.placeholder(tf.float32, shape=(n_inputs, n_hidden1))
original_biases = tf.placeholder(tf.float32, shape=(n_hidden1))
assign_hidden1_weights = tf.assign(hidden1_weights, original_weights)
assign_hidden1_biases = tf.assign(hidden1_biases, original_biases)

init = tf.global_variables_initializer()

with tf.Session() as sess:
	sess.run(init)
	sess.run(assign_hidden1_weights, feed_dict={original_weights: original_w})
	sess.run(assign_hidden1_biases, feed_dict={original_biases: original_b})
	[...] # Train the model on your new task
```

### 冻住低神经层

第一个DNN的底层可能已经学会了检测图片中的低级特征，这些特征在两个图像分类任务中都很有用，因此可以按照原模型重新使用这些图层。在训练新的DNN时，“冻结”权重通常是一个好主意：如果下层权重是固定的，那么更高层的权重将更容易训练（因为他们不必学习移动目标）。 期间冻结较低层训练，最简单的解决方案是给优化器提供要训练的变量列表，不包括来自较低层的变量：

```python
train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                               scope="hidden[34]|outputs")
training_op = optimizer.minimize(loss, var_list=train_vars)
```

第一行获得隐藏层3、4和输出层的所有的训练变量。排除隐藏层1和2.下一步提供这些变量给优化器的**minimize()**方法.现在层1和2被冻住了：训练时不会改动（叫冻结层）。

### 缓存冻结层

由于冻结图层不会更改，因此可以为每个训练实例缓存最顶层冻结图层的输出。由于训练会遍历整个数据集很多次，因此您将只需要在每个训练实例中进行一次冻结层（而不是每个时期一次），这将为您提供巨大的速度提升。 例如，您可以先通过较低层运行整个训练集（假设您有足够的RAM）：

```
hidden2_outputs = sess.run(hidden2, feed_dict={X: X_train})
```

然后训练时，不是建立训练实例的批次，而是从隐藏层2输出构建batch，然后喂给训练操作：

```python
import numpy as np
n_epochs = 100
n_batches = 500

for epoch in range(n_epochs):
    shuffled_idx = rnd.permutation(len(hidden2_outputs))
	hidden2_batches = np.array_split(hidden2_outputs[shuffled_idx], n_batches)
	y_batches = np.array_split(y_train[shuffled_idx], n_batches)
	for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):
		sess.run(training_op, feed_dict={hidden2: hidden2_batch, y: y_batch})
```

最后一行运行之前定义的训练操作（冷冻层1和2），然后喂给她从第二隐藏层输出的一个batch（还有batch的targets（y)）.由于我们给tf隐藏层2的输出，它不会再去评估它（或者它依赖的任何节点）。


