## 调整，丢弃或者替换上个神经层

原始模型的输出层应该被替换，因为它对于新的任务是最没有用的，并且它也没有新任务正确的输出数。

相似的，原始模型较上层的隐藏层要比较低层的来的没用，因为对新任务来说，最有用的高级特征和原始模型的高级特征可能完全不搭嘎。你想找到能复用的正确层数。

试着将所有拷贝层都冻住，然后训练你的模型，看效果如何，然后试着解冻一个或两个顶层，让反向传播调整他们然后看效果有么有提升，你的训练数据越多，你能解冻的层数越多。

如果你仍得不到好效果。并且你只有一点点训练数据。那试着丢掉顶层隐藏层，然后冻住剩下的隐藏层。可以这样迭代直到找到合适的层数。如果你有大量的训练数据，你可以试着替换顶隐藏层而不是丢弃他们，甚至添加更多隐藏层。

### 模型动物园

在哪可以找到一哥类似你要解决的任务的训练好的神经网络呢？首先要看的是你模型目录。这是保存所有你的模型和组织他们的好理由。另一个是在**模型动物园**里搜索。许多人为许多任务训练机器学习模型并善意的公布预训练模型。

tf有它自己的模型动物园在[这里](https://github.com/tensorflow/models).特别的，它包含了大多数先进的图形分类网络如VGG，Inception和ResNet。包括代码，预训练模型和下载流行图形数据集的工具。

另一个流行的模型动物园是Caffe的[Model Zoo](https://github.com/BVLC/caffe/wiki/Model-Zoo),它也包含了许多计算机视觉模型（LeNet,AlexNet,ZFNet,GoogleNet,VGGNet,inception)在多个数据集训练（ImageNet,Places Database CIFAR10等)。Saumitro Dasgupta 写了一个转换器，在[点这里](https://github.com/ethereon/caffe-tensorflow )

### 无监督预训练

假设你要处理一个复杂任务，没有很多标记过的训练数据。但不幸的是，你找不到一个训练过类似任务的模型.。别放弃！首先你应该收集更多标记过的训练数据，但这很难或者代价很大，你仍然可以执行无监督的预训练。如果你有许多没有标记的训练数据，你可以尝试一个接一个的训练神经层，从最低层开始，然后向上，使用一个无监督特征检测算法如**Restricted Boltzmann Machines **(RBM)或者autoencoders。每层都在上一层训练过的图层输出上进行训练（除了正在训练的图层，其他图层都被冻结）。一旦以这种方式训练所有层，就可以使用监督室学习（即反向传播）对网络进行微调。

这是相当长和乏味的过程，但效果不错。事实上，这是Geoffrey Hinton  和他的团队在2006年使用的技术并导致了神经网络的复兴和深度学习的成功。直到2010年，无监督预训练（通常使用RBM)是深度网络的常用的，它直到梯度消失问题得到缓解后，纯使用反向传播的DNN才更为常用。然而，无监督预训练（现在更通常用autoencoders而不是RBM）对于有个没有类似模型可以使用并且只有小部分标记的训练数据但是大量的无标记训练数据的复杂任务仍是 个好选择。


![1530242467.png](https://upload-images.jianshu.io/upload_images/3509189-6e5f4fea680947e9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

### 在一个辅助任务预训练

最后一个选择是在辅助任务上训练第一个神经网络，可以轻松获取或生成标记的训练数据，然后复用该网络的较低层。第一个神经网络的低层会学习可能被第二神经网络复用的特征检测器？。

比如，如果你想构建一个识别人脸的系统，你可能只有每个人的照片，显然不足以训练一个好的分类器。搜集每个人的数百张图片也不实际。然而你可以在网上收集很多人的照片，并训练第一个神经网络来检测两张不同图片是否具有同一个人的特征。这样的神经网络会学习有关人脸的好特征，这样，使用它的较低层可以让你使用很少的训练数据能得到很好的人脸识别器。

收集无标记的训练数据相当容易，但标记他们代价很高。这种情况下，一个常用技术是标记你所有的样本成“好",然后通过破坏好的实例来生成许多新的训练实例，并将这些被破坏的实例标记为“坏”。然后你可以训练第一个神经网络来分类好和坏。比如，你可以下载大量的句子，然后标记为”好“，然后随机的改变每个句子中的词，然后标记为“坏”。如果一个神经网络（以后简称神网）可以识别出”The dog sleeps"是好句子，而“The dog they"是坏句子，那它大概知道语言的一些东西。复用它的较低层在许多语言处理任务上会很有用。

另一种方法是训练第一网络输出每个训练实例的分数，然后使用损失函数来保证好实例的分数比坏实例高一些。这被称为**最大边缘学习**。

### 加速优化器

训练一个非常大的深神网会相当的慢。至今我们已经见识了四种加速训练的方法（并且得到一个好解决方案）：为连接权重应用一个好初始化策略，使用好的激活函数，使用撮归一化，和复用一个预训练网络的一部分。另一个加速方式是使用比梯度下降优化器更快的优化器。这里会讲最流行的：Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, 和 Adam optimization. 

本节的结论是直接用Adam optimization，如果你不关心它是如何工作的，那么将**GradientDescentOptimizer **替换为**AdamOptimizer **，然后直接跳到下一节！只要这一小小改变，训练会变快好几倍。然而Adam optimization有三个超参数你可以调整（加上学习率），默认值通常效果不错，但如果你想修改他们，最好知道他们是做什么的比较好。Adam optimization结合了来自其他优化算法的几个想法，所以首先先看下这些算法是有必要的。

### Momentum optimization（动量优化） 

想象一下在一个光滑的斜坡上放一个保龄球：它刚开始挺慢的，然后会快速加速直到达到一个最大速度（如果有摩擦或空气阻力）。这是Momentum优化背后的非常简单的想法。相比下，梯度下降则是简单的沿着斜坡进行小步有规则的距离，因此需要更多的时间到达底部。

回想一下，梯度下降法简单地通过直接减去成本函数J（θ）相对于权重（∇θJ（θ））乘以学习率η的梯度来更新权重θ。等式 θ ← θ - η∇θJ(θ) 。它不关心之前的梯度是多少，如果局部梯度很小，那么它前进的很慢。

动量优化关心的是以前的梯度是什么：在每次迭代中，它将局部梯度添加到动量矢量m（乘以学习速率η），并且通过简单地减去该动量矢量来更新权重。见下图。换句话说，梯度被用作加速度，而不是速度。为了模拟某种摩擦机制并防止动量过大，该算法引入了一个新的超参数β，简称**momentum**，它必须设置在0（高摩擦）和1（无摩擦）之间。典型的动量值为0.9.

![1530252694.png](https://upload-images.jianshu.io/upload_images/3509189-10a0484155ac4b27.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


可以简单的验证，如果梯度保持不变，最终速度（权重更新最大值）等于梯度乘上学习率η 乘上1/(1-β)。例如，当β=0.9时，最终速度等于十倍的梯度乘以学习率。所以动量优化最终比梯度下降快十倍。这让动量优化在逃离高原上比梯度下降更快。梯度下降在陡峭的坡上下降的特别快，但是之后到达山谷要花很长时间。相对的，动量优化将越来越快的滑下谷底，直到到达底部（最佳）。在深神网中没有使用Batch Normalization，较上层往往会得到不同尺寸的输入，因此使用动量优化会有很大帮助，可以帮助越过局部最优。

由于momentum，优化器可能会超过点，然后再回来，然后超过，这样摇摆多次，直到稳定到最小值，这是为什么在系统中有点摩擦的原因之一：它消除了这些振荡，从而加速了收敛。

在tf中实现动量优化是一件易如反掌的事：只需将**GradientDescentOptimizer**替换为**MomentumOptimizer **。

```python
optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,
                                       momentum=0.9)
```

动量优化一个不好的点是添加了另一个超参数要调整。但是，0.9的动量值通常在实践中运行良好，几乎总是比梯度下降更快。

## Nesterov Accelerated Gradient

动量优化的一个变体总是比动量优化来的更快。Nesterov Momentum optimization  或者叫Nesterov Accelerated Gradient （NAG），是测量损失函数的梯度不是在局部位置而是动量方向的前面一点位置。和动量优化唯一不同的是梯度是在θ + βm    测量而不是在θ上。

![1530254207192.png](https://upload-images.jianshu.io/upload_images/3509189-b5871fb0cb665795.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



这个小改动有用是因为通常动量向量将指向正确的方向（最优方向），因此使用在该方向测量的更远的梯度而不是在原始位置上使用梯度来的更精确些。

![1530254408163.png](https://upload-images.jianshu.io/upload_images/3509189-846bf18c2029fabf.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

其中∇1表示在起点θ处测量的成本函数的梯度，并且∇2表示位于θ+βm的点处的梯度。

正如你所看到的，Nesterov更新最终略微接近最佳状态。一段时间后，这些小改进加起来，NAG最终比常规的动量优化来得快。此外，当动量推动权重穿过山谷时，∇1    继续推进穿过山谷，而∇2退回谷底，这有助于减少振荡，从而更快收敛。

与常规的动量优化相比，NAG几乎总能加快训练速度，要使用它只需在创建MomentumOptimizer 时设置**use_nesterov=True**：

```
optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9,
use_nesterov=True)
```

## AdaGrad

再考虑下拉长碗问题：梯度下降快速的在最陡峭的坡上下降，然后在谷底变慢。如果算法能尽早检测到并修正它的方向指向全局最优，会更好。

[AdaGrad 算法](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)实现了这个，通过在最陡的维度缩放梯度向量：

![1530255257175.png](https://upload-images.jianshu.io/upload_images/3509189-6c4db179d5163040.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


第一步将梯度的平方积累进矢量中（⊗ 表示元素方面的乘法）。这个相当于为矢量s的每个元素计算si ← si + (∂ / ∂ θi J(θ))^2 。换句话说，每个si关于参数θi累加成本函数的偏导数的平方。 如果成本函数沿第i维陡峭，则在每次迭代中，si将变得越来越大。

第二步几乎和梯度下降完全相同，但有一个大的不同：梯度向量缩小根号（s+ϵ），（⊘    代表单元除，ϵ是一个平滑项，避免被0除，通常设为10^-10)。

简单说就是这个算法腐蚀学习率，但对于陡峭的坡，其速度要快于具有温和斜坡的坡。这叫**适应的学习率**。一个额外的好处是它不怎么需要调整学习率的超参数η 。

AdaGrad在简单的二次问题上表现不错，但在训练神经网络的时候经常很早停止训练，学习率缩小太多，算法在到达全局最优前完全停止。所以即使tf有一个**AdagradOptimizer**，但你不要拿它训练神网（它对如线性回归这样简单任务适用).





