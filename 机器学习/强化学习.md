

强化学习（RL）是当今最激动人心的机器学习领域之一，也是最古老的学习领域之一。它自20世纪50年代以来一直存在，多年来产生了许多有趣的应用，特别是在游戏中（例如，TD-Gammon，五子棋游戏程序）和机器控制，但很少成为头条新闻。但是在2013年发生了一场革命，当时一家名为DeepMind的英国创业公司的研究人员展示了一个[可以0基础学习玩任何Atari游戏的系统](https://arxiv.org/pdf/1312.5602v1.pdf)，最终在大多数情况下超越人类，仅使用原始像素作为输入且不需要任何关于游戏规则的知识，这是一系列令人惊叹的壮举中的第一个，最终在2016年3月以他们的AlphaGo系统对抗围棋世界冠军李世石的胜利而告终。没有任何程序能够击败这个游戏大师，更不用说世界冠军了。今天，RL的整个领域沸腾了新的想法，具有广泛的应用。 2014年，谷歌以超过5亿美元的价格收购了DeepMind。

那他们是怎么做到的？ 事后看来它似乎相当简单：他们将深度学习的力量应用到强化学习领域，并且它超越了他们最疯狂的梦想。 在本章中，我们将首先解释强化学习是什么以及它擅长什么，然后我们将介绍深度强化学习中最重要的两种技术：**策略梯度**和**深度Q-网络（DQN）**,包括对马尔可夫决策过程（MDP）的讨论。 我们将使用这些技术来训练模型来平衡移动车上的杆和另一个玩Atari游戏。 相同的技术可用于各种任务，从步行机器人到自动驾驶汽车。

## 学会优化奖励

在强化学习中，软件**代理**在**环境**中进行**观察**并采取**行动**，作为回报，它会收到**奖励**。 其目标是学会以最大化其预期的长期奖励的方式行事。 如果你不介意一些拟人化，你可以把积极的奖励视为快乐，将负面的奖励视为痛苦（在这种情况下，“奖励”一词有点误导）。 简而言之，代理在环境中行动并通过反复试验来学习，以最大限度地提高其乐趣并最大限度地减少其痛苦。

这是一个非常广泛的设置，可以应用于各种各样的任务。 以下是一些示例（参见图16-1）：

- 代理可以是控制步行机器人的程序。 在这种情况下，环境是现实世界，代理通过一组传感器（如摄像头和触摸传感器）观察环境，其动作包括发送信号以激活电机。 它可以被编程为每当它接近目标目的地时获得正奖励，并且每当它浪费时间，朝错误方向或跌倒时获得负奖励。
- 代理人可以是控制Pac-Man女士的程序。 在这种情况下，环境是Atari游戏的模拟，动作是九个可能的操纵杆位置（左上，下，中等），观察是截图，奖励只是游戏点
- 类似地，代理可以是玩棋盘游戏的程序，例如围棋
- 代理人不必控制物理（或虚拟）移动的东西。 例如，它可以是智能恒温器，在接近目标温度时获得奖励并节省能量，在人类需要调整温度时获得负奖励，因此代理必须学会预测人类需求
- 代理商可以观察股票市场价格并决定每秒买入或卖出多少。 奖励显然是货币收益和损失。

![1530771380(1).png](https://upload-images.jianshu.io/upload_images/3509189-9ee9a2f7efe8911b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


请注意，可能根本没有任何积极的奖励; 例如，代理人可能会在迷宫中四处走动，在每个时间步都获得负面奖励，因此最好尽快找到出口！ 还有许多其他任务的例子，其中强化学习非常适合，例如自动驾驶汽车，在网页上放置广告，或控制图像分类系统应该集中注意力的位置。

## 策略检索

软件代理用于确定其操作的算法称为其**策略**。 例如，该策略可以是一个神经网络，将观察结果作为输入并输出要采取的动作（见图16-2）。

![1530772407(1).png](https://upload-images.jianshu.io/upload_images/3509189-e97c34eb12fd1f52.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

策略可以是您可以想到的任何算法，甚至不必是确定性的。 例如，考虑一个机器人吸尘器，其奖励是它在30分钟内吸收的灰尘量。 它的政策可能是以每秒一些概率p向前推进，或者以概率1-p随机向左或向右旋转。 旋转角度是-r和+ r之间的随机角度。 由于该策略涉及一些随机性，因此称为**随机策略**。 机器人将具有不稳定的轨迹，这保证它最终将到达它可以到达的任何地方并拾取所有灰尘。 问题是：它会在30分钟内吸收多少灰尘？

你会如何训练这样的机器人？ 你可以调整两个策略参数：概率p和角度范围r。 一种可能的学习算法可能是为这些参数尝试许多不同的值，并选择性能最佳的组合（见图16-3）。 这是策略搜索的一个例子，在这种情况下使用蛮力方法。 但是，当**策略空间**太大时（通常就是这种情况），以这种方式找到一组好的参数就像海底捞针一样。

探索策略空间的另一种方法是使用**遗传算法**。 例如，您可以随机创建第一代100个策略并尝试它们，然后“杀死”80个最差的策略6并使20个幸存者每个产生4个后代。

后代只是其父的副本加上一些随机变量。 幸存的策略加上他们的后代共同组成了第二代。 您可以通过这种方式继续迭代，直到找到一个好的策略。

![1530772540(1).png](https://upload-images.jianshu.io/upload_images/3509189-753e5a023bc46504.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


另一种方法是使用优化技术，通过评估关于策略参数的奖励的梯度，然后通过遵循向更高奖励的梯度（梯度上升）来调整这些参数。 这种方法称为策略梯度（PG），我们将在本章后面详细讨论。 例如，回到真空吸尘器机器人，您可以略微增加p并评估是否会增加机器人在30分钟内拾取的灰尘量; 如果确实如此，那么再增加p，否则减少p。 我们将使用TensorFlow实现流行的PG算法，但在我们开始之前，我们需要为代理创建一个环境，因此是时候介绍OpenAI gym了。

## OpenAI Gym简介

强化学习的挑战之一是，为了训练代理，首先需要有一个工作环境。 如果你想编写一个学习玩Atari游戏的代理，你将需要一个Atari游戏模拟器。 如果您想对步行机器人进行编程，那么环境就是现实世界，您可以直接在该环境中训练您的机器人，但这有其局限性：如果机器人从悬崖上掉下来，您不能单击“撤消”。”

你也无法加快时间; 增加更多的计算能力不会让机器人移动得更快。 并行训练1,000个机器人通常太昂贵了。 简而言之，在现实世界中，训练是艰难而缓慢的，因此您通常需要一个模拟环境至少进行训练

OpenAI gym是一个工具包，提供各种模拟环境（Atari游戏，棋盘游戏，2D和3D物理模拟等），因此您可以训练代理，比较它们或开发新的RL算法

我们来安装OpenAI gym。 对于最小的OpenAI gym安装，只需使用pip：

```
$ pip3 install --upgrade gym
```

接下来打开一个Python shell或一个Jupyter并创建你的第一个环境：

```
>>> import gym
>>> env = gym.make("CartPole-v0")
[2016-10-14 16:03:23,199] Making new env: MsPacman-v0
>>> obs = env.reset()
>>> obs
array([-0.03799846, -0.03288115, 0.02337094, 0.00720711])
>>> env.render()
```

*make()*函数创建一个环境，在本例中为CartPole环境。 这是一个2D模拟，其中可以向左或向右加速推车，以平衡放置在其顶部的杆（参见图16-4）。 创建环境后，我们必须使用*reset()*方法对其进行初始化。 这将返回第一个观察结果。 观察取决于环境类型。 对于CartPole环境，每个观测值都是包含四个浮点数的1D NumPy数组：这些浮点数表示滑块的水平位置（0.0 =中心），其速度，极点角度（0.0 =垂直）以及角速度。 最后，render（）方法显示环境，如图16-4所示。

![CartPole游戏](http://upload-images.jianshu.io/upload_images/3509189-28f6648821229101.gif?imageMogr2/auto-orient/strip)

![1530772791(1).png](https://upload-images.jianshu.io/upload_images/3509189-c1141bcb32565794.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如果希望*render()*将渲染图像作为NumPy数组返回，可以将mode参数设置为rgb_array（注意其他环境可能支持不同的模式）：

```
>>> img = env.render(mode="rgb_array")
>>> img.shape # height, width, channels (3=RGB)
(400, 600, 3)
```

不幸的是，即使将模式设置为“rgb_array”，CartPole（以及其他一些环境）也会将图像渲染到屏幕上。 避免这种情况的唯一方法是使用假的X服务器，如Xvfb或Xdummy。 例如，您可以使用以下命令安装Xvfb并启动Python：

```
xvfb-run -s“ - screen 0 1400x900x24” python
```
或者使用[xvfbwrapper](https://github.com/cgoldberg/xvfbwrapper)包。
让我们问一下环境可能采取的行动：

```
>>> env.action_space
Discrete(2)
```

Discrete（2）意味着可能的动作是整数0和1，它们表示加速左（0）或右（1）。 其他环境可以具有更多离散动作或其他类型的动作（例如，连续的）。 由于杆向右倾斜，让我们向右加速滑块：

```
>>> action = 1 # accelerate right
>>> obs, reward, done, info = env.step(action)
>>> obs
array([-0.03865608, 0.16189797, 0.02351508, -0.27801135])
>>> reward
1.0
>>> done
False
>>> info
{}
```

step()方法执行给定的操作并返回四个值：

obs :

​	这是新观察。滑块现在向右移动（obs [1]> 0）。 杆仍然向右倾斜（obs [2]> 0），但其角速度现在为负（obs [3] <0），因此在下一步之后它可能会向左倾斜

reward :

​	在这种环境中，无论你做什么，每一步都会获得1.0的奖励，因此我们的目标是尽可能长时间地保持运行。

done 

​	当这一次结束时，该值将为True。 当杆子倾斜太多时会发生这种情况。 之后，必须重置环境才能再次使用。

info 

​	该字典可以在其他环境中提供额外的调试信息。 这些数据不应用于训练（这将是作弊）。

让我们硬编码一个简单的策略，当极点向左倾斜时加速离开，当极点向右倾斜时加速。 我们将运行此政策，以查看超过500次的平均奖励：

```
def basic_policy(obs):
	angle = obs[2]
	return 0 if angle < 0 else 1
	
totals = []
for episode in range(500):
	episode_rewards = 0
	obs = env.reset()
	# 1000 steps max, we don't want to run forever
	for step in range(1000): 
		action = basic_policy(obs)
		obs, reward, done, info = env.step(action)
		episode_rewards += reward
		if done:
			break
	totals.append(episode_rewards)
```

```
# result
>>> import numpy as np
>>> np.mean(totals), np.std(totals), np.min(totals), np.max(totals)
(42.125999999999998, 9.1237121830974033, 24.0, 68.0)
```

即使有500次尝试，这项策略也从未设法保持杆位直立超过68个连续步骤。 不太好。 如果你看一下[Jupyter notebook](https://github.com/ageron/handson-ml)中的模拟，你会发现滑块左右摆动越来越强烈，直到杆子倾斜太多。 让我们看看神经网络是否能够提出更好的策略。

## 神经网络策略

让我们创建一个神经网络策略。 就像我们之前硬编码的策略一样，这个神经网络将观察作为输入，它将输出要执行的动作。 更准确地说，它将估计每个动作的概率，然后我们将根据估计的概率随机选择一个动作（见图16-5）。 对于CartPole环境，只有两个可能的动作（左或右），所以我们只需要一个输出神经元。 它将输出动作0的概率p（左），当然动作1（右）的概率为1 - p。

例如，如果它输出0.7，那么我们将选择具有70％概率的动作0，以及具有30％概率的动作1。

![1530774728(1).png](https://upload-images.jianshu.io/upload_images/3509189-acee45918737b1cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

z.z
你可能想知道为什么我们根据神经网络给出的概率选择随机动作，而不是仅选择具有最高分数的动作。 这种方法可以让代理在探索新操作和利用已知运行良好的操作之间找到适当的平衡点。 这是一个类比：假设你第一次去餐厅，所有的菜肴看起来都很吸引人，所以你随机选择一个。 如果事实证明是好的，你可以增加下次点可能性，但你不应该把这个概率增加到100％，否则你永远不会尝试其他菜肴，其中一些甚至 比你试过的那个可能更好。

另请注意，在此特定环境中，可以安全地忽略过去的操作和观察，因为每个观察都包含环境的完整状态。 如果存在某种隐藏状态，那么您可能还需要考虑过去的行动和观察。 例如，如果环境仅显示推车的位置而不显示其速度，则不仅要考虑当前观察，还要考虑先前的观察以估计当前速度。 另一个例子是观察结果很嘈杂; 在这种情况下，您通常希望使用过去几次观察来估计最可能的当前状态。 CartPole问题因此很简单; 观察结果是无噪声的，它们包含环境的完整状态.

以下是使用TensorFlow构建此神经网络策略的代码：

```
import tensorflow as tf
from tensorflow.contrib.layers import fully_connected

# 1. Specify the neural network architecture
n_inputs = 4 # == env.observation_space.shape[0]
n_hidden = 4 # it's a simple task, we don't need more hidden neurons
n_outputs = 1 # only outputs the probability of accelerating left
initializer = tf.contrib.layers.variance_scaling_initializer()

# 2. Build the neural network
X = tf.placeholder(tf.float32, shape=[None, n_inputs])
hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,
weights_initializer=initializer)
logits = fully_connected(hidden, n_outputs, activation_fn=None,
weights_initializer=initializer)
outputs = tf.nn.sigmoid(logits)

# 3. Select a random action based on the estimated probabilities
p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])
action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)
init = tf.global_variables_initializer()
```

我们来看看这段代码：

1. 在导入之后，我们定义了神经网络架构。 输入的数量是观察空间的大小（在CartPole为4的情况下），我们只有四个隐藏单位而不需要更多，我们只有一个输出概率（向左移动的概率）。
2. 接下来我们构建神经网络。 在这个例子中，它是一个带有单个输出的香草多层感知器。 请注意，输出层使用逻辑（sigmoid）激活函数，以便输出0.0到1.0的概率。 如果有两个以上可能的动作，每个动作将有一个输出神经元，你将使用softmax激活函数。
3. 最后，我们调用multinomial（）函数来选择随机动作。 在给定每个整数的对数概率的情况下，该函数独立地对一个（或多个）整数进行采样。 例如，如果用数组[np.log（0.5），np.log（0.2），np.log（0.3）]和num_samples = 5调用它，那么它将输出五个整数，每个整数都有 50％概率为0,20％为1，30％为2.在我们的例子中，我们只需要一个表示要采取行动的整数。 由于输出张量仅包含向左移动的概率，我们必须首先将1输出连接到它以具有包含左右动作概率的张量。 请注意，如果有两个以上可能的操作，则神经网络必须为每个操作输出一个概率，因此您不需要连接步骤。

好的，我们现在有一个神经网络策略，它将采取观察和输出动作。 但我们如何训练呢？

## 评估行动：信贷分配问题

如果我们知道每个步骤的最佳动作是什么，我们可以像往常一样训练神经网络，通过最小化估计概率和目标概率之间的交叉熵。 这只是定期监督学习。 然而，在强化学习中，代理获得的唯一指导是通过奖励，奖励通常是稀疏和延迟的。 例如，如果代理设法平衡100个步骤的极点，那么它怎么知道它所采取的100个动作中的哪个是好的，哪个是坏的？ 所有这一切都知道，在最后一次行动之后杆子倒下了，但是最后一次行动肯定不是完全负责的。 这被称为信用分配问题：当代理获得奖励时，很难知道哪些行为应该被记入（或归咎于）。 想想一只狗在表现良好后数小时获得奖励; 它会理解它的回报吗？

为了解决这个问题，一个共同的策略是根据之后所有奖励的总和来评估一个行动，通常在每一步都应用贴现率r。例如（见图16-6），如果一个代理人决定连续三次走右并在第一步之后获得+10奖励，在第二步之后获得0，最后在第三步之后获得-50，然后假设我们使用折扣率r = 0.8，第一个动作的总得分为10 + r×0 + r2×（ - 50）= -22。如果贴现率接近0，那么与即时奖励相比，未来的奖励将不会太多。相反，如果贴现率接近1，那么远期到未来的奖励几乎同样多
作为直接奖励。典型折扣率为0.95或0.99。折扣率为0.95，未来13步奖励大约是即时奖励的一半（自0.9513≈0.5），而折扣率为0.99，奖励未来69步将立即减半奖励。在CartPole环境中，操作具有相当短期的影响，因此选择0.95的折扣率似乎是合理的。

![1530775237(1).png](https://upload-images.jianshu.io/upload_images/3509189-98345fcc1a4e365b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


当然，一个好的动作之后可能会有几个不良行为导致极点迅速下降，导致好的动作获得低分（同样，一个优秀的演员可能有时会出演一部可怕的电影）。 但是，如果我们玩足够多次游戏，平均而言，好的行为会比不好的行动得分更好。 因此，为了获得相当可靠的动作分数，我们必须运行多个剧集并将所有动作分数标准化（通过减去均值并除以标准差）。 在那之后，我们可以合理地假设具有负分数的动作是差的，而具有正分数的动作是好的。 完美 - 现在我们有办法评估每个行动，我们准备使用政策梯度培训我们的第一个代理人。 我们来看看如何。

## 政策梯度

如前所述，PG算法通过跟随梯度向更高的奖励来优化策略的参数。 一种流行的PG算法，称为REINFORCE算法，由Ronald Williams于19929年推出。 这是一个常见的变体：

1. 首先，让神经网络策略多次玩游戏，并在每一步计算可能使所选动作更有可能的渐变，但不要应用这些渐变。
2. 运行多集后，计算每个动作的分数（使用前一段中描述的方法）
3. 如果某个动作的分数为正，则表示该动作是好的，并且您希望应用之前计算的渐变以使该动作更有可能在将来被选中。 但是，如果得分为负，则表示操作很糟糕，并且您希望应用相反的渐变来使此操作在将来稍微不太可能。 解决方案是简单地将每个梯度向量乘以相应的动作得分。
4. 最后，计算所有得到的梯度向量的平均值，并使用它来执行梯度下降步骤。

让我们使用TensorFlow实现这个算法。 我们将训练我们之前建立的神经网络策略，以便它学会平衡推车上的杆。 让我们首先完成我们之前编码的构建阶段，以添加目标概率，成本函数和训练操作。 由于我们的行为就好像所选择的动作是最好的动作一样，如果所选动作是动作0（左），目标概率必须是1.0，如果是动作1（右），则目标概率必须是0.0：

```
y = 1. - tf.to_float(action)
```

现在我们有了目标概率，我们可以定义成本函数（交叉熵）并计算梯度：

```
learning_rate = 0.01
cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(
labels=y, logits=logits)
optimizer = tf.train.AdamOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(cross_entropy)
```

请注意，我们正在调用优化器的compute_gradients（）方法而不是minimize（）方法。 这是因为我们想在应用渐变之前调整渐变.10 compute_gradients（）方法返回一个渐变矢量/变量对列表（每个可训练变量一对）。 让我们将所有渐变放在一个列表中，以便更方便地获取它们的值：

```
gradients = [grad for grad, variable in grads_and_vars]
```

好的，现在是棘手的部分。 在执行阶段，算法将运行策略，并在每个步骤评估这些梯度张量并存储它们的值。 在一些剧集之后，它将调整这些渐变，如前所述（即，将它们乘以动作分数并将它们归一化）并计算经调整的渐变的平均值。 接下来，它需要将生成的渐变反馈给优化器，以便它可以执行优化步骤。 这意味着每个梯度向量需要一个占位符。 此外，我们必须创建将应用更新的渐变的操作。 为此，我们将调用优化器的apply_gradients（）函数，该函数获取梯度向量/变量对的列表。 我们将给它一个包含更新梯度的列表（即通过梯度占位符提供的梯度），而不是给它原始的梯度向量：

```
gradient_placeholders = []
grads_and_vars_feed = []
for grad, variable in grads_and_vars:
	gradient_placeholder = tf.placeholder(tf.float32,
	shape=grad.get_shape())
	gradient_placeholders.append(gradient_placeholder)
	grads_and_vars_feed.append((gradient_placeholder, variable))
	
training_op = optimizer.apply_gradients(grads_and_vars_feed)
```

让我们退一步看看完整的施工阶段：

```
n_inputs = 4
n_hidden = 4
n_outputs = 1
initializer = tf.contrib.layers.variance_scaling_initializer()

learning_rate = 0.01

X = tf.placeholder(tf.float32, shape=[None, n_inputs])
hidden = fully_connected(X, n_hidden, activation_fn=tf.nn.elu,
weights_initializer=initializer)
logits = fully_connected(hidden, n_outputs, activation_fn=None,
weights_initializer=initializer)
outputs = tf.nn.sigmoid(logits)
p_left_and_right = tf.concat(axis=1, values=[outputs, 1 - outputs])
action = tf.multinomial(tf.log(p_left_and_right), num_samples=1)

y = 1. - tf.to_float(action)
cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(
labels=y, logits=logits)
optimizer = tf.train.AdamOptimizer(learning_rate)
grads_and_vars = optimizer.compute_gradients(cross_entropy)
gradients = [grad for grad, variable in grads_and_vars]
gradient_placeholders = []
grads_and_vars_feed = []
for grad, variable in grads_and_vars:
	gradient_placeholder = tf.placeholder(tf.float32,
	shape=grad.get_shape())
	gradient_placeholders.append(gradient_placeholder)
	grads_and_vars_feed.append((gradient_placeholder, variable))

training_op = optimizer.apply_gradients(grads_and_vars_feed)

init = tf.global_variables_initializer()
saver = tf.train.Saver()
```

到执行阶段！ 我们需要一些函数来计算总折扣奖励，给定原始奖励，并在多个剧集中规范化结果：

```
def discount_rewards(rewards, discount_rate):
	discounted_rewards = np.empty(len(rewards))
	cumulative_rewards = 0
	for step in reversed(range(len(rewards))):
		cumulative_rewards = rewards[step] + cumulative_rewards
		* discount_rate
		discounted_rewards[step] = cumulative_rewards
	return discounted_rewards

def discount_and_normalize_rewards(all_rewards, discount_rate):
	all_discounted_rewards = [discount_rewards(rewards)
	for rewards in all_rewards]
	flat_rewards = np.concatenate(all_discounted_rewards)
	reward_mean = flat_rewards.mean()
    reward_std = flat_rewards.std()
	return [(discounted_rewards - reward_mean)/reward_std
			for discounted_rewards in all_discounted_rewards]
```

让我们检查一下这是否有效：

```
>>> discount_rewards([10, 0, -50], discount_rate=0.8)
array([-22., -40., -50.])
>>> discount_and_normalize_rewards([[10, 0, -50], [10, 20]], discount_rate=0.8)
[array([-0.28435071, -0.86597718, -1.18910299]),
array([ 1.26665318, 1.0727777 ])]
```

对discount_rewards（）的调用完全返回我们期望的内容（参见图16-6）。您可以验证函数discount_and_normalize_rewards（）确实返回两个剧集中每个动作的规范化分数。 请注意，第一集比第二集差得多，因此其标准化分数均为负数; 第一集中的所有动作都将被视为不良，相反，第二集中的所有动作都将被认为是好的。

我们现在拥有培训政策所需的一切：

```
n_iterations = 250 # number of training iterations
n_max_steps = 1000 # max steps per episode
n_games_per_update = 10 # train the policy every 10 episodes
save_iterations = 10 # save the model every 10 training iterations
discount_rate = 0.95

with tf.Session() as sess:
	init.run()
	for iteration in range(n_iterations):
		all_rewards = [] # all sequences of raw rewards for each
		episode
		all_gradients = [] # gradients saved at each step of each episode
		for game in range(n_games_per_update):
			current_rewards = [] # all raw rewards from the current episode
			current_gradients = [] # all gradients from the current episode
			obs = env.reset()
			for step in range(n_max_steps):
				action_val, gradients_val = sess.run(
				[action, gradients],
				feed_dict={X: obs.reshape(1, n_inputs)}) # one obs
obs, reward, done, info = env.step(action_val[0][0])
				current_rewards.append(reward)
				current_gradients.append(gradients_val)
				if done:
					break
			all_rewards.append(current_rewards)
			all_gradients.append(current_gradients)
# At this point we have run the policy for 10 episodes, and we are
# ready for a policy update using the algorithm described earlier.
all_rewards = discount_and_normalize_rewards(all_rewards)
feed_dict = {}
for var_index, grad_placeholder in enumerate(gradient_placeholders):
# multiply the gradients by the action scores, and compute the mean
	mean_gradients = np.mean(
	[reward * all_gradients[game_index][step][var_index]
	for game_index, rewards in enumerate(all_rewards)
	for step, reward in enumerate(rewards)],axis=0)
	feed_dict[grad_placeholder] = mean_gradients

sess.run(training_op, feed_dict=feed_dict)
if iteration % save_iterations == 0:
	saver.save(sess, "./my_policy_net_pg.ckpt")
```

每次训练迭代都是通过运行10集的策略开始的（每集最多1000步，以避免永远运行）。 在每一步，我们还计算渐变，假装所选择的动作是最好的。 在这10集之后，我们使用discount_and_normalize_rewards（）函数计算动作得分; 我们遍历所有剧集和所有步骤中的每个可训练变量，将每个梯度向量乘以其相应的动作得分; 我们计算得到的梯度的平均值。 最后，我们运行训练操作，为它们提供这些平均梯度（每个可训练变量一个）。 我们还每10次培训操作保存模型。

我们完成了！ 此代码将训练神经网络策略，它将成功学习平衡推车上的杆（您可以在Jupyter笔记本中试用它）。 请注意，代理商实际上有两种方式可以输掉游戏：杆子可能会倾斜太多，或者推车可能完全脱离屏幕。 通过250次训练迭代，该政策学会了很好地平衡杆位，但它还不足以避免离开屏幕。 几百次训练迭代将解决这个问题

研究人员试图找到即使代理最初对环境一无所知也能很好地工作的算法。 但是，除非您正在撰写论文，否则您应该尽可能多地将先前知识注入代理，因为它将大大加快培训速度。 例如，您可以添加与屏幕中心距离和极点角度成比例的负奖励。 此外，如果您已经有一个相当好的策略（例如，硬编码），您可能需要训练神经网络来模仿它，然后再使用策略渐变来改进它。

尽管相对简单，但该算法非常强大。 您可以使用它来解决比平衡推车上的杆更难的问题。 事实上，AlphaGo基于类似的PG算法（加上蒙特卡罗树搜索，这超出了本书的范围）。

我们现在将看另一个流行的算法系列。 虽然PG算法直接尝试优化策略以增加奖励，但我们现在将看到的算法不那么直接：代理学会估计每个州的预期未来贴现奖励的预期总和，或者每个州的预期贴现未来奖励的预期总和。 每个州的行动，然后利用这些知识来决定如何行动。 要理解这些算法，我们必须首先介绍马尔可夫决策过程（MDP）。

## 马尔可夫决策过程

在20世纪初期，数学家安德烈·马尔科夫研究了没有记忆的随机过程，称为马尔可夫链。 这样的过程具有固定数量的状态，并且在每个步骤中从一个状态随机地演变为另一个状态。 它从状态s演变到状态s'的概率是固定的，它只取决于对（s，s'），而不取决于过去的状态（系统没有记忆）。

图16-7显示了具有四种状态的马尔可夫链的示例。 假设该过程在状态s0开始，并且有70％的可能性将在下一步保持该状态。 最终它必然会离开那个状态并且永远不会回来，因为没有其他州指向s0。 如果它进入状态s1，则它很可能进入状态s2（概率为90％），然后立即回到状态s1（概率为100％）。 它可能在这两个状态之间交替多次，但最终它将落入状态s 3并永远保持在那里（这是一个终端状态）。 马尔可夫链可以具有非常不同的动力学，它们大量用于热力学，化学，统计学等等。

![1530776134(1).png](https://upload-images.jianshu.io/upload_images/3509189-5ed1cbb72f486eeb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


马尔可夫决策过程最初是在20世纪50年代由理查德·贝尔曼描述的.11它们类似于马尔可夫链，但有一个转折点：在每一步，一个代理人可以选择几种可能的行动之一，转移概率取决于所选择的行动。 此外，一些州过渡会返回一些奖励（正面或负面），而代理人的目标是找到一种能够随着时间的推移最大化奖励的政策。

例如，图16-8中表示的MDP在每个步骤中具有三种状态和最多三种可能的离散动作。如果它在状态s0开始，则代理可以在动作a0，a1或a2之间进行选择。如果它选择动作a1，它只是确定地保持在状态s0，并且没有任何奖励。因此，它可以决定永远留在那里，如果它想要的话。但如果它选择动作a0，它有70％的概率获得+10的奖励，并保持在状态s0。然后它可以一次又一次地尝试获得尽可能多的奖励。但有一次，它最终会在状态s1中结束。在状态s1中，它只有两个可能的动作：a0或a1。它可以通过反复选择动作a1选择保持不变，或者它可以选择继续进入状态s2并获得-50（ouch）的负奖励。在状态s3中，除了采取行动a1之外别无选择，这很可能导致它回到状态s0，在途中获得+40的奖励。你得到了照片。通过查看此MDP，您能猜出哪种策略会随着时间的推移获得最大回报吗？在状态s0中很明显，动作a0是最佳选择，并且在状态s3中，代理除了采取动作a1之外别无选择，但是在状态s1中，代理是否应保持放置（a0）或通过火（a2）。

![1530776277(1).png](https://upload-images.jianshu.io/upload_images/3509189-840308a8927673fa.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


贝尔曼找到了一种估算任何状态s的最优状态值的方法，记为V *（s），它是代理人在达到状态s后平均预期的所有贴现的未来奖励的总和，假设它是最佳的。 他表明，如果代理最佳地运行，那么应用Bellman最优性方程（见公式16-1）。 这个递归方程表明，如果代理人的行为是最佳的，那么当前状态的最佳值等于在采取一个最佳行动后平均得到的回报，加上该行为可导致的所有可能的下一个状态的预期最优值。。

![1530776380(1).png](https://upload-images.jianshu.io/upload_images/3509189-b2150aa5e2a39418.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


该等式直接导致可以精确估计每个可能状态的最佳状态值的算法：首先将所有状态值估计初始化为零，然后使用值迭代算法迭代更新它们（参见公式16-2）。 一个显着的结果是，如果有足够的时间，这些估计值将保证收敛到最优状态值，对应于最优策略。

![1530776431(1).png](https://upload-images.jianshu.io/upload_images/3509189-6cfa9502527ad014.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


该算法是动态规划的一个例子，它将一个复杂的问题（在这种情况下，估计可能无限的贴现未来奖励总和）分解成易处理的子问题，可以迭代地解决（在这种情况下找到最大化平均值的行动） 奖励加上打折的下一个州值）。

了解最佳状态值可能很有用，尤其是评估策略，但它并不会明确告诉代理要做什么。 幸运的是，Bellman发现了一种非常相似的算法来估计最佳状态 - 动作值，通常称为Q值。 状态 - 动作对（s，a）的最佳Q值，记为Q *（s，a），是代理在达到状态s后平均可以预期的贴现未来奖励的总和并选择动作a， 但是在它看到这个动作的结果之前，假设它在该动作之后发挥最佳作用。

以下是它的工作原理：再次，您首先将所有Q值估计值初始化为零，然后使用Q值迭代算法更新它们（参见公式16-3）。

![1530776512(1).png](https://upload-images.jianshu.io/upload_images/3509189-b9d05f66f1ea02f7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


一旦获得了最佳Q值，定义最优策略（注意π*（s））是微不足道的：当代理处于状态s时，它应该选择具有该状态的最高Q值的动作：π* s = argmaxQ *（s，a）。

让我们将此算法应用于图16-8中所示的MDP。 首先，我们需要定义MDP：

```
nan=np.nan # represents impossible actions
T = np.array([ # shape=[s, a, s']
[[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]],
[[0.0, 1.0, 0.0], [nan, nan, nan], [0.0, 0.0, 1.0]],
[[nan, nan, nan], [0.8, 0.1, 0.1], [nan, nan, nan]],
])
R = np.array([ # shape=[s, a, s']
[[10., 0.0, 0.0], [0.0, 0.0, 0.0], [0.0, 0.0, 0.0]],
[[10., 0.0, 0.0], [nan, nan, nan], [0.0, 0.0, -50.]],
[[nan, nan, nan], [40., 0.0, 0.0], [nan, nan, nan]],
])
possible_actions = [[0, 1, 2], [0, 2], [1]]

```

现在让我们运行Q-Value Iteration算法：

```
Q = np.full((3, 3), -np.inf) # -inf for impossible actions
for state, actions in enumerate(possible_actions):
Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions
learning_rate = 0.01
discount_rate = 0.95
n_iterations = 100
for iteration in range(n_iterations):
Q_prev = Q.copy()
for s in range(3):
for a in possible_actions[s]:
Q[s, a] = np.sum([
T[s, a, sp] * (R[s, a, sp] + discount_rate * np.max(Q_prev[sp]))
for sp in range(3)
])
```

生成的Q值看起来像这样:

```
>>> Q
array([[ 21.89498982, 20.80024033, 16.86353093],
[ 1.11669335, -inf, 1.17573546],
[ -inf, 53.86946068, -inf]])
>>> np.argmax(Q, axis=1) # optimal action for each state
array([0, 2, 1])
```

这给了我们这个MDP的最优策略，当使用0.95的贴现率时：在状态s0中选择动作a0，在状态s1中选择动作a2（通过火！），在状态s2中选择动作a1（唯一可能的） 行动）。 有趣的是，如果您将贴现率降低到0.9，则最优政策会发生变化：在状态s1中，最佳操作变为a0（保持不变;不要经历火灾）。 这是有道理的，因为如果你比现在更重视现在，那么未来奖励的前景不值得立即痛苦。

## 时间差异学习和Q学习

强化离散动作的学习问题通常可以建模为马尔可夫决策过程，但是代理最初不知道转移概率是什么（它不知道T（s，a，s'）），并且它不知道什么是 奖励将是（它不知道R（s，a，s'））。 它必须经历每个州和每个过渡至少一次以了解奖励，并且如果要对转移概率进行合理估计，它必须多次经历它们

时间差异学习（TD学习）算法与值迭代算法非常相似，但调整后考虑到代理仅具有部分MDP知识的事实。 通常，我们假设代理最初只知道可能的状态和操作，仅此而已。 代理使用探索策略（例如，纯粹随机的策略）来探索MDP，并且随着它的进展，TD学习算法基于实际观察到的转变和奖励来更新状态值的估计（参见等式16-4）

![1530776749(1).png](https://upload-images.jianshu.io/upload_images/3509189-e14430e2e6fdd243.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


TD Learning与Stochastic Gradient Descent有许多相似之处，特别是它一次处理一个样本。 就像SGD一样，只有逐渐降低学习率才能真正收敛（否则它会在最佳状态下反弹）...

对于每个状态s，此算法只是跟踪代理在离开该状态时获得的直接奖励的运行平均值，以及它预期稍后得到的奖励（假设它最佳地运行）

类似地，Q-Learning算法是Q值迭代算法适应转换概率和奖励最初未知的情况（见公式16-5）。

![1530776820(1).png](https://upload-images.jianshu.io/upload_images/3509189-77a833bd07f5827f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


对于每个状态 - 动作对（s，a），该算法跟踪代理在离开具有动作a的状态s时获得的奖励的运行平均值，以及它期望稍后获得的奖励。 由于目标政策将采取最佳行动，因此我们对下一个州采取最大Q值估计。

以下是Q-Learning的实施方式：

```
import numpy.random as rnd
learning_rate0 = 0.05
learning_rate_decay = 0.1
n_iterations = 20000
s = 0 # start in state 0
Q = np.full((3, 3), -np.inf) # -inf for impossible actions
for state, actions in enumerate(possible_actions):
Q[state, actions] = 0.0 # Initial value = 0.0, for all possible actions
for iteration in range(n_iterations):
a = rnd.choice(possible_actions[s]) # choose an action (randomly)
sp = rnd.choice(range(3), p=T[s, a]) # pick next state using T[s, a]
reward = R[s, a, sp]
learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)
Q[s, a] = learning_rate * Q[s, a] + (1 - learning_rate) * (
reward + discount_rate * np.max(Q[sp])
)
s = sp # move to next state
```

给定足够的迭代，该算法将收敛到最佳Q值。 这称为o ??策略算法，因为正在训练的策略不是正在执行的策略。 有点令人惊讶的是，这种算法能够通过随机观察代理人行为来学习最优政策（想象当你的老师是醉酒的猴子时学会打高尔夫球）。 我们可以做得更好吗？

## 勘探政策

当然，只有在探索政策彻底探索MDP时，Q-Learning才能发挥作用。 虽然保证纯粹随机的政策最终会多次访问每个州和每个过渡，但这可能需要很长时间才能完成。 因此，更好的选择是使用ε-贪婪策略：在每一步中它以概率ε随机地行动，或者贪婪地（选择具有最高Q值的动作）以概率1-ε行动。 ε-贪婪政策（与完全随机的政策相比）的优势在于它将花费越来越多的时间来探索环境的有趣部分，因为Q值估计变得越来越好，同时仍然花费一些时间来访问 MDP的未知区域。 从ε（例如1.0）的高值开始然后逐渐减小它（例如，低至0.05）是很常见的。

或者，不是依赖于探索的机会，另一种方法是鼓励探索政策尝试以前没有尝试过的行动。 这可以作为添加到Q值估计的奖励来实现，如公式16-6所示

![1530776960(1).png](https://upload-images.jianshu.io/upload_images/3509189-3435e8cac5015c03.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## 近似Q-Learning

Q-Learning的主要问题是它不能很好地扩展到具有许多状态和动作的大型（甚至中等）MDP。 考虑尝试使用Q-Learning训练代理人扮演Pac-Man女士。 Pac-Man女士可以吃250粒以上的颗粒，每颗都可以存在或不存在（即已经吃过）。 所以可能状态的数量大于2250≈1075（并且考虑到颗粒的可能状态）。 这比可观察宇宙中的原子更多，因此绝对没有办法跟踪每个Q值的估计值.

解决方案是使用可管理数量的参数找到近似Q值的函数。 这称为近似Q-Learning。 多年来，建议使用从状态中提取的手工制作的特征的线性组合（例如，最近的重影的距离，它们的方向等）来估计Q？值，但DeepMind表明使用深度神经网络可以工作 更好，特别是对于复杂的问题，它不需要任何功能工程。 用于估计Q值的DNN称为深度Q-网络（DQN），并且使用DQN进行近似Q-学习称为深度Q-学习。

在本章的其余部分，我们将使用Deep Q-Learning训练代理人扮演Pac-Man女士，就像DeepMind在2013年所做的那样。代码可以很容易地调整，以学习很好地玩大部分的Atari游戏。 它可以在大多数动作游戏中实现超人技能，但在具有长期故事情节的游戏中并不是那么擅长。

## 学习玩Pac-Man女士使用深度Q学习

由于我们将使用Atari环境，因此我们必须首先安装OpenAI gym的Atari依赖项。 在我们处理它的同时，我们还将为您可能想要使用的其他OpenAI健身房环境安装依赖项。 在macOS上，假设您已经安装了Homebrew，则需要运行：

在Ubuntu上，键入以下命令（如果您使用的是Python 2，则用python替换python3）：

```
$ apt-get install -y python3-numpy python3-dev cmake zlib1g-dev libjpeg-dev\
xvfb libav-tools xorg-dev python3-opengl libboost-all-dev libsdl2-dev swig
```

然后安装额外的Python模块：

```
$ pip3 install --upgrade 'gym[all]'
```

如果一切顺利，您应该能够创建一个Pac-Man女士环境：

```
>>> env = gym.make("MsPacman-v0")
>>> obs = env.reset()
>>> obs.shape # [height, width, channels]
(210, 160, 3)
>>> env.action_space
Discrete(9)
```

如您所见，有九个离散动作可用，它们对应于操纵杆的九个可能位置（左，右，上，下，中，左上等），观察结果只是Atari的截图 屏幕（见图16-9，左），表示为3D NumPy数组。 这些图像有点大，因此我们将创建一个小的预处理功能，将裁剪图像并将其缩小到88×80像素，将其转换为灰度，并提高Pac-Man女士的对比度。 这将减少DQN所需的计算量，并加快培训速度。

```
mspacman_color = np.array([210, 164, 74]).mean()
def preprocess_observation(obs):
img = obs[1:176:2, ::2] # crop and downsize
img = img.mean(axis=2) # to greyscale
img[img==mspacman_color] = 0 # improve contrast
img = (img - 128) / 128 - 1 # normalize from -1. to 1.
return img.reshape(88, 80, 1)
```

![1530777201(1).png](https://upload-images.jianshu.io/upload_images/3509189-a56f7a813bc61691.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


接下来，让我们创建DQN。 它可以只使用状态 - 动作对（s，a）作为输入，并输出相应Q值Q（s，a）的估计值，但由于动作是离散的，因此使用神经网络更方便 仅采用状态s作为输入，并且每个动作输出一个Q值估计。 DQN将由三个卷积层组成，接着是两个完全连接的层，包括输出层（见图16-10）。

![1530777273(1).png](https://upload-images.jianshu.io/upload_images/3509189-951b36b3332ea1e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


正如我们将要看到的，我们将使用的训练算法需要两个具有相同架构（但参数不同）的DQN：一个用于在训练期间驱动Pac-Man女士（演员），另一个将观看演员和 学习它的试验和错误（评论家）。 我们会定期将评论家复制给演员。 由于我们需要两个相同的DQN，我们将创建一个q_network（）函数来构建它们：

```
from tensorflow.contrib.layers import convolution2d, fully_connected
input_height = 88
input_width = 80
input_channels = 1
conv_n_maps = [32, 64, 64]
conv_kernel_sizes = [(8,8), (4,4), (3,3)]
conv_strides = [4, 2, 1]
conv_paddings = ["SAME"]*3
conv_activation = [tf.nn.relu]*3
n_hidden_in = 64 * 11 * 10 # conv3 has 64 maps of 11x10 each
n_hidden = 512
hidden_activation = tf.nn.relu
n_outputs = env.action_space.n # 9 discrete actions are available
initializer = tf.contrib.layers.variance_scaling_initializer()
def q_network(X_state, scope):
prev_layer = X_state
conv_layers = []
with tf.variable_scope(scope) as scope:
for n_maps, kernel_size, stride, padding, activation in zip(
conv_n_maps, conv_kernel_sizes, conv_strides,
conv_paddings, conv_activation):
prev_layer = convolution2d(
prev_layer, num_outputs=n_maps, kernel_size=kernel_size,
stride=stride, padding=padding, activation_fn=activation,
weights_initializer=initializer)
conv_layers.append(prev_layer)
last_conv_layer_flat = tf.reshape(prev_layer, shape=[-1, n_hidden_in])
hidden = fully_connected(
last_conv_layer_flat, n_hidden, activation_fn=hidden_activation,
weights_initializer=initializer)
outputs = fully_connected(
hidden, n_outputs, activation_fn=None,
weights_initializer=initializer)
trainable_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
scope=scope.name)
trainable_vars_by_name = {var.name[len(scope.name):]: var
for var in trainable_vars}
return outputs, trainable_vars_by_name
```

该代码的第一部分定义了DQN架构的超参数。 然后q_network（）函数创建DQN，将环境的状态X_state作为输入，并使用变量作用域的名称。 请注意，我们将只使用一个观察来表示环境的状态，因为几乎没有隐藏状态（除了闪烁的对象和鬼魂的方向）

trainable_vars_by_name字典收集此DQN的所有可训练变量。 当我们创建操作以将评论家DQN复制到演员DQN时，它将在一分钟内有用。 字典的键是变量的名称，剥去与作用域名称对应的前缀部分。 它看起来像这样：

```
>>> trainable_vars_by_name
{'/Conv/biases:0': <tensorflow.python.ops.variables.Variable at 0x121cf7b50>,
'/Conv/weights:0': <tensorflow.python.ops.variables.Variable...>,
'/Conv_1/biases:0': <tensorflow.python.ops.variables.Variable...>,
'/Conv_1/weights:0': <tensorflow.python.ops.variables.Variable...>,
'/Conv_2/biases:0': <tensorflow.python.ops.variables.Variable...>,
'/Conv_2/weights:0': <tensorflow.python.ops.variables.Variable...>,
'/fully_connected/biases:0': <tensorflow.python.ops.variables.Variable...>,
'/fully_connected/weights:0': <tensorflow.python.ops.variables.Variable...>,
'/fully_connected_1/biases:0': <tensorflow.python.ops.variables.Variable...>,
'/fully_connected_1/weights:0': <tensorflow.python.ops.variables.Variable...>}
```

现在让我们创建输入占位符，两个DQN，以及将评论家DQN复制到actor DQN的操作：

```
X_state = tf.placeholder(tf.float32, shape=[None, input_height, input_width,
input_channels])
actor_q_values, actor_vars = q_network(X_state, scope="q_networks/actor")
critic_q_values, critic_vars = q_network(X_state, scope="q_networks/critic")
copy_ops = [actor_var.assign(critic_vars[var_name])
for var_name, actor_var in actor_vars.items()]
copy_critic_to_actor = tf.group(*copy_ops)
```

让我们退一步：我们现在有两个DQN，它们都能够将环境状态（即预处理观察）作为输入，并输出该状态下每个可能动作的估计Q值。 另外，我们有一个名为copy_critic_to_actor的操作，用于将评论家DQN的所有可训练变量复制到演员DQN。 我们使用TensorFlow的tf.group（）函数将所有赋值操作分组到一个方便的操作中。

演员DQN可以用来扮演Pac-Man女士（最初非常糟糕）。 如前所述，您希望它足够彻底地探索游戏，因此您通常希望将其与ε-贪婪的政策或其他探索策略相结合

但是评论家DQN怎么样？ 它将如何学习玩游戏？ 简短的回答是，它将尝试使其Q值预测与演员通过其游戏体验估计的Q值相匹配。 具体来说，我们将让演员玩一会儿，将其所有经验存储在重播记忆中。 每个记忆将是5元组（状态，动作，下一状态，奖励，继续），其中“继续”项目在游戏结束时将等于0.0，否则为1.0。 接下来，我们将定期从重放存储器中采集一批存储器，并从这些存储器中估算Q值。 最后，我们将训练评论家DQN使用常规监督学习技术预测这些Q值。 每经过几次训练迭代，我们就会将评论家DQN复制到演员DQN。 就是这样！ 公式16-7显示了用于训练评论家DQN的成本函数：

![1530777480(1).png](https://upload-images.jianshu.io/upload_images/3509189-ee61b23a4646c05c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


重播内存是可选的，但强烈推荐。 没有它，你将使用可能非常相关的连续体验训练评论家DQN。 这会引入很多偏差并减慢训练算法的收敛速度。 通过使用重放存储器，我们确保馈送到训练算法的存储器可以是相当不相关的。

让我们添加评论家DQN的培训操作。 首先，我们需要能够为内存批处理中的每个状态操作计算其预测的Q值。 由于DQN为每个可能的动作输出一个Q值，我们只需要保留与该存储器中实际选择的动作相对应的Q值。 为此，我们将动作转换为单热矢量（回想一下，除了第i个索引处的1之外，这是一个满0的向量），并将其乘以Q值：这将使所有Q-归零 除了与记忆动作相对应的值之外的值。 然后仅对第一轴求和以仅获得每个存储器的期望Q值预测。

```
X_action = tf.placeholder(tf.int32, shape=[None])
q_value = tf.reduce_sum(critic_q_values * tf.one_hot(X_action, n_outputs),
axis=1, keep_dims=True)
```

接下来让我们添加训练操作，假设目标Q值将通过占位符提供。 我们还创建了一个名为global_step的非连续变量。 优化器的minimize（）操作将负责增加它。 另外，我们创建了常用的init操作和Saver。

```
y = tf.placeholder(tf.float32, shape=[None, 1])
cost = tf.reduce_mean(tf.square(y - q_value))
global_step = tf.Variable(0, trainable=False, name='global_step')
optimizer = tf.train.AdamOptimizer(learning_rate)
training_op = optimizer.minimize(cost, global_step=global_step)
init = tf.global_variables_initializer()
saver = tf.train.Saver()
```

那就是施工阶段。 在我们查看执行阶段之前，我们需要一些工具。 首先，让我们从实现重放内存开始。 我们将使用一个双端队列表，因为它非常有效地将项目推送到队列，并在达到最大内存大小时从列表末尾弹出它们。 我们还将编写一个小函数来从重放内存中随机抽取一批经验.

```
from collections import deque
replay_memory_size = 10000
replay_memory = deque([], maxlen=replay_memory_size)
def sample_memories(batch_size):
indices = rnd.permutation(len(replay_memory))[:batch_size]
cols = [[], [], [], [], []] # state, action, reward, next_state, continue
for idx in indices:
memory = replay_memory[idx]
for col, value in zip(cols, memory):
col.append(value)
cols = [np.array(col) for col in cols]
return (cols[0], cols[1], cols[2].reshape(-1, 1), cols[3],
cols[4].reshape(-1, 1))
```

接下来，我们需要演员来探索游戏。 我们将使用ε-greedy策略，并在50,000个训练步骤中逐渐将ε从1.0减少到0.05：

```
eps_min = 0.05
eps_max = 1.0
eps_decay_steps = 50000

def epsilon_greedy(q_values, step):
epsilon = max(eps_min, eps_max - (eps_max-eps_min) * step/eps_decay_steps)
if rnd.rand() < epsilon:
return rnd.randint(n_outputs) # random action
else:
return np.argmax(q_values) # optimal action

```

而已！ 我们拥有开始培训所需的一切。 执行阶段不包含任何太复杂的东西，但它有点长，所以深呼吸。 准备？ 我们走吧！ 首先，让我们初始化一些变量：

```
n_steps = 100000 # total number of training steps
training_start = 1000 # start training after 1,000 game iterations
training_interval = 3 # run a training step every 3 game iterations
save_steps = 50 # save the model every 50 training steps
copy_steps = 25 # copy the critic to the actor every 25 training steps
discount_rate = 0.95
skip_start = 90 # skip the start of every game (it's just waiting time)
batch_size = 50
iteration = 0 # game iterations
checkpoint_path = "./my_dqn.ckpt"
done = True # env needs to be reset
```

接下来，让我们打开会话并运行主训练循环：

```
with tf.Session() as sess:
if os.path.isfile(checkpoint_path):
saver.restore(sess, checkpoint_path)
else:
init.run()
while True:
step = global_step.eval()
if step >= n_steps:
break
iteration += 1
if done: # game over, start again
obs = env.reset()
for skip in range(skip_start): # skip the start of each game
obs, reward, done, info = env.step(0)
state = preprocess_observation(obs)
# Actor evaluates what to do
q_values = actor_q_values.eval(feed_dict={X_state: [state]})
action = epsilon_greedy(q_values, step)
# Actor plays
obs, reward, done, info = env.step(action)
next_state = preprocess_observation(obs)
# Let's memorize what just happened
replay_memory.append((state, action, reward, next_state, 1.0 - done))
state = next_state

if iteration < training_start or iteration % training_interval != 0:
continue
# Critic learns
X_state_val, X_action_val, rewards, X_next_state_val, continues = (
sample_memories(batch_size))
next_q_values = actor_q_values.eval(
feed_dict={X_state: X_next_state_val})
max_next_q_values = np.max(next_q_values, axis=1, keepdims=True)
y_val = rewards + continues * discount_rate * max_next_q_values
training_op.run(feed_dict={X_state: X_state_val,
X_action: X_action_val, y: y_val})
# Regularly copy critic to actor
if step % copy_steps == 0:
copy_critic_to_actor.run()
# And save regularly
if step % save_steps == 0:
saver.save(sess, checkpoint_path)
```

如果存在检查点文件，我们首先恢复模型，否则我们只是正常初始化变量。然后主循环开始，其中迭代计算自程序启动以来我们经历的游戏步骤的总数，并且步骤计算自训练开始以来的训练步骤的总数（如果恢复检查点，则还恢复全局步骤） ）。然后代码重置游戏（并跳过第一个无聊的游戏步骤，没有任何反应）。接下来，演员评估要做什么，并玩游戏，并将其体验记忆在重放记忆中。然后，定期（在预热期后），评论家进行训练。它对一批存储器进行采样，并要求演员估计下一个状态的所有动作的Q值，并应用公式16-7计算目标Q值Y_val。这里唯一棘手的部分是我们必须将下一个状态的Q值乘以连续向量，将对应于游戏结束的记忆的Q值归零。接下来，我们进行一项培训操作，以提高评论家预测Q值的能力。最后，我们定期将评论家复制给演员，然后保存模型。

不幸的是，训练非常缓慢：如果你使用笔记本电脑进行训练，那么Pac-Man女士可能需要几天才能获得任何好处，如果你看一下学习曲线，测量每集的平均奖励，你会注意到它 非常吵。 在某些时候，很长一段时间内可能没有明显的进展，直到突然间代理人学会在合理的时间内存活。 如前所述，一种解决方案是尽可能多地将先前知识注入到模型中（例如，通过预处理，奖励等），并且您还可以尝试通过首先训练模型以模仿基本策略来引导模型。 在任何情况下，RL仍然需要相当多的耐心和调整，但最终的结果是非常令人兴奋的.
