### 撮归一化

虽然使用He初始化和ELU（或任意ReLU变体）可以在训练的一开始显著减少梯度下降、爆炸问题，但不能保证训练期间不会出现。

在2015年，Sergey Ioffe 和 Christian Szegedy  提出一种技术叫**撮正规化（Batch Normalization（BN））**来处理梯度消失和爆炸问题，更一般地说是每层输入的分布变化的问题在训练期间，随着前面层的参数改变而改变（他们称之为内部协变量问题）。

该技术包括在每层的激活函数之前，简单的对输入进行零中心和归一化，然后进行缩放，并使用每层2个新参数来移动结果（一个用于缩放，一个用转移）。换句话说，这个操作可以让模型学习最优的尺度和平均每层的输入。

为了零中心和归一化输入，算法需要判断输入的平均值和标准差。它通过计算当前小撮输入的平均值和标准差来判断）。


![1530169861.png](https://upload-images.jianshu.io/upload_images/3509189-0b599d2c5e5a762b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


- μB是经验均值，在整个小撮B中评估
- σB 是经验标准差，也是在整个小撮中评估
- mB是小撮中的实例数
- x(i) 是零中心和归一化输入
- γ是层的缩放参数  
- β是层的转移参数
- ϵ 是一个很小的数，以避免被0除（通常为10^-3）。
- z(i)是BN操作的输出：输入的缩放和移位的版本。

测试的时候，没有小撮来计算经验平均数和标准差，你可以使用所有训练集的平均数和标准差。这些通常在训练过程中使用移动的平均值进行有效计算。因此，总共为每个批次标准化层学习四个参数：γ（标度），β（偏移量），μ（平均值）和σ（标准偏差）。

作者证明了这项技术改进了他们尝试的所有深度神经网络。梯度消失问题减少了，因此他们可以使用饱和激活函数如tanh甚至是logistic激活函数。网络也对权重初始化较为不敏感。能够使用更大的学习率，显著的提升学习过程。特别的，他们说，应用到最先进的图像分类模型中，BN使得在相同准确率下使训练步数减少了14倍，并且远远的击败原始模型。

Batch Normalization然而为模型添加了些复杂性（尽管它消除了自第一个隐藏层要做归一化输入数据的必要，只要它是撮归一化就可以）。此外还有运行惩罚，神经网络由于每层额外的计算使得预测变慢。因此，如果你想要闪电般快速预测，你可能要检查下再使用BN前，普通的ELU+He初始化效果如何。

### 使用tf实现bn

tf提供一个方法batch_normalization()简化中心和归一化输入，但你一定得自己算平均值和标准差（基于训练时的小撮数据或者测试时在整个数据集）然后作为参数，传到这个方法中，并且你也必须处理创建缩放和平移参数（并传给这个方法）。这可行，但是不是最简便的方法。你应该使用batch_norm()方法，这个为你做了以上事情，你可直接调用，或者让fully_connected()方法使用它。

```python
import tensorflow as tf
from tensorflow.contrib.layers import batch_norm

n_inputs = 28 * 28
n_hidden1 = 300
n_hidden2 = 100
n_outputs = 10

X = tf.placeholder(tf.float32, shape=(None, n_inputs), name="X")
is_training = tf.placeholder(tf.bool, shape=(), name='is_training')
bn_params = {
	'is_training': is_training,
	'decay': 0.99,
	'updates_collections': None
}
hidden1 = fully_connected(X, n_hidden1, scope="hidden1",
normalizer_fn=batch_norm, normalizer_params=bn_params)

hidden2 = fully_connected(hidden1, n_hidden2, scope="hidden2",
normalizer_fn=batch_norm, normalizer_params=bn_params)

logits = fully_connected(hidden2, n_outputs, activation_fn=None,scope="outputs",
normalizer_fn=batch_norm, normalizer_params=bn_params)
```

is_training 这个告诉batch_norm是否要用当前的小撮数据的平均值和标准差，还是用整个测试集计算平均值。

下一步我们定义bn_params，要传入batch_norm的参数，算法使用指数衰减计算平均值。给定一个新值v，运行平均值v^通过等式 v^ <-v^x decay + v x (1 - decay)。一个好的decay值通常趋近于1比如，0.9,0.99,0.999等（对于大数据集用更多的9，更小的小撮）。最后updates_collections设置为None，如果你想要batch_norm()方法来更新运行平均值在训练时执行batch normalization。如果没有设置这个参数，默认情况下，TensorFlow将只添加这些操作，将运行平均值更新为您必须运行的操作集合。

最后在fully_connected()方法中使用batch_norm()方法，让它在调用激活函数前执行normalize。

注意到，默认batch_norm()只有center，normalize，和shift输入；它没有缩放他们（γ为1）。这对于没有激活功能的图层或者使用ReLU激活功能是有意义的，因为下一图层的权重可以处理缩放，但对于任何其他激活功能，您应该添加“scale”：
True 到bn_params

定义前面三层相当重复，因为几个参数都是一样的。为了避免重复一次又一次的相同参数，你可以使用**arg_scope()**创建一个参数范围,第一个参数是方法列表，其他参数会自动的添加到这些方法中。

```python
with tf.contrib.framework.arg_scope(
	[fully_connected],
    normalizer_fn=batch_norm,
    normalizer_params=bn_params):
    
    hidden1 = fully_connected(X, n_hidden1, scope="hidden1")
    hidden2 = fully_connected(hidden1, n_hidden2, scope="hidden2")
    logits = fully_connected(hidden2, n_outputs, scope="outputs",
activation_fn=None)
```

当有10层并且想设置激活函数，初始化器，归一化器，正规化器等等的时候，就会让代码可读性更高。

接下来构建期和之前一样：定义损失函数，创建优化器，让他最小化损失函数，定义估值操作，创建Saver等等。

执行期也差不多。当你运行一个依赖于batch_norm层的操作，你需要设置is_training 为True或者False。

```python
with tf.Session as sess:
    sess.run(init)
    
    for epoch in range(n_epochs):
        [...]
        for X_batch,y_batch in zip(X_batches,y_batches):
            sess.run(training_op,
                    feed_dict={is_training:True,X:X_batch,y:y_batch})
        accuracy_score = accuracy.eval(
        feed_dict={is_training:False,X:X_test_scaled,y:y_test})
        print(accuracy_score)
```

### 梯度剪

一个流行的技术来减少梯度爆炸问题是简单的在反向传播时剪掉梯度，这样他们就不会超过某个阈值（常用于RNN）。现在人们倾向于BN，但了解梯度裁剪以及如何实现它仍然很有用。

在tf，优化器的minimize()方法处理计算梯度和应用他们。所以，你必须首先调用优化器的compute_gradients()方法，然后使用clip_by_value()方法创造一个操作来剪梯度。最后使用优化器的apply_gradients()方法创造一个方法在应用剪过的梯度：

```python
threshold = 1.0
optimizer = tf.train.GradientDescentOptimizer(learning_rate)

grads_and_vars = optimizer.compute_gradients(loss)

capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]

training_op = optimizer.apply_gradients(capped_gvs)
```

你可以在每步训练时运行training_op操作，它会计算梯度，裁剪他们到-1.0到1.0之间，然后应用他们。threshold可以调整。

