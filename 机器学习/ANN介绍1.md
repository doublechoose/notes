鸟启发我们飞行，但飞机不拍打翅膀，同样的，ANN也和生物神经不一样。ANN是深度学习的核心。他们是万金油、强大并且可拓展的。让他们可以处理高度复杂的机器学习任务。

### 从生物到人造神经网络

ANN的概念从1943年就有了。神经生理学家和数学家提出了一种简单的计算模型表示生物神经在动物脑中处理复杂计算。从那时起，我们将看到许多其他体系结构的发明。

早期ANN的成功让我们相信近期内会让机器充满智慧，但当被确认暂时无法实现时，研究进入了一段黑暗期。然后在1980年早期ANN新的架构提出后又回春了一段时间，但是到了1990年代，研究员偏向于强大的机器学习技术如支持向量机，因为他们提供了更好的结果和强大的理论基础。最后我们见证了另一波ANN潮流。这波潮流会和之前的一样褪去吗？这里有些理由值得相信这一次不一样，并且对我们生活的影响更甚：

- 现在有大量的数据可用于训练神经网络，并且ANN在大型复杂问题上做的比其他ML技术要好。
- 从1990开始的快速增长的计算力让训练大型神经网络在合理时间内完成变得可能，这有部分是因为摩尔定律，也需要感谢造出强大的GPU的游戏公司。
- 改进了训练算法。虽然只和90年代使用有点不同，但是这些小改动却有巨大的积极影响。
- 人工智能网络的一些理论局限性在实践中证明是良性的。许多人认为ANN会陷入局部最优，但事实证明这在实践中十分罕见（或者与全局最优离得很近）
- ANN似乎进入了资金和进步的良性循环。基于ANN的神奇产品会占领头条新闻，这样会引起更多关注和资金流向他们，导致更多的进步，然后更多神奇的产品。

### Perceptron 

Perceptron 是最简单ANN架构，基于一个稍微不同的人造神经元：线性阈值单元（LTU）：输入和输出是数字（不是二元值），每个输入都与一个权值相关联。LTU计算其输入的加权和（z = w1 x1 + w2 x2 + ⋯ + wn xn = wT · x ），然后对该和使用阶跃函数并输出结果。

一个LTU可以被用于简单线性二元分类。它计算输入的线性联合，如果结果超过阈值，它输出阳性，否则输出阴性（就像logistic回归或者线性SVM）。

Perceptron是单一层LTU的组合，每个神经元 连接所有的输入。

输入神经元：将输入的输出，不做处理。此外会添加一个偏差(x0  =1).这个偏差用一个叫**bias neuron** 表示，只输出1.

两个神经元之间的连接重量是只要他们有相同的产出就会增加。更具体的说，Perceptron一次接受一个训练实例，并且对每个实例进行预测。对每个产生错误预测的输出神经元，会强化能造成预测成功的神经元之间的连接。

每个输出神经元的决策边界是线性的，所以Perceptron不能处理复杂模式（和logistic回归分类器一样）。然而如果训练实例是线性可分的，那么这个算法会收敛到一个解决方案，这叫感知机收敛定理。

Scikit-Learn提供了一个Perceptron类，实现了一个单一LTU网络。这可以做很多事情，比方说测试鸢尾花数据集：

```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris()
X = iris.data[:,(2,3)]# petal length petal width
y = (iris.target == 0).astype(np.int)

per_clf = Perceptron(random_state = 42)
per_clf.fit(X,y)

y_pred = per_clf.predict([2,0.5])
```

Perceptron 是不是和随机梯度下降法很像？实际上，Scikit-learn的Perceptron相当于具有以下超参数的SGDClassifier: loss = "perceptron",learning_rate="constant",eta0=1， penalty = None

与logistic回归分类相反，Perceptron不输出类别概率，它只是基于一个硬阈值进行预测。这是倾向于对感知器进行Logistic回归的好理由。

在1969年，一些人提出了Perceptron的许多短板，特别是无法处理一些琐碎的问题（包括或（异或）分类问题），当然对任何线性分类器模型都有这个问题（logistic回归分类器也是）。但是因为研究员对Perceptron期待越大，失望越大，于是很多研究员都完全放弃了联结主义学说（神经网络的研究）有利于更高层次的问题，如逻辑，问题解决和搜索。

然而，结果是Perceptron的一些限制可以通过堆栈多个Perceptron得到消除。得到的ANN叫多层感知机（Multi-Layer Perceptron）。值得一说的是，MLP可以解决XOR问题。

### 多层感知机和反向传播

MLP由一个输入层，一个或多个LTU（隐藏层），和一个最后的LTU（输出层）组成。每层（除了输出层）包含一个bias 神经元，并且全连接到下一层。当一个ANN有2个或更多的隐藏层时，这个ANN就不叫ANN了，叫DNN（deep neural network）。

许多年，学者们都没有找到训练MLP的方法。直到1986年，D. E. Rumelhart等人发表了一篇介绍**backpropagation**（反向传播）训练算法。今天可以将其描述为梯度下降使用反向模式进行autodiff下降。

对于每个训练实例，算法将其输入网络并计算每个连续层中每个神经元的输出（这是向前传送，就像预测那样）。然后它测量网络的输出误差（期望输出和实际输出之间的差），然后计算最后隐藏层中每个神经元对每个输出的贡献的误差。然后计算这些误差有哪些是从前一个隐藏层中贡献的-以此类推直到达到输入层。这个反向通道通过在网络中向后传播错误梯度高效的计算了错误梯度的连接权重。

更简单的说：对于每个训练实例，反向传播算法先做一个预测（正向通过），测量错误，然后通过每一层反向测量每个连接的误差贡献（反向通过），最后稍微调整连接权重以减少错误（梯度下降步骤）。

为了让这算法正确工作，对MLP架构做了一个重要调整：将step函数换成logistic函数σ(z) = 1 / (1 + exp(–z)) 。logistic函数有非零导数，允许梯度下降在每步进行调整。反馈算法可以用其他阶跃函数替代logistic函数，2个常用阶跃函数：

Te hyperbolic tangent function tanh (z) = 2σ(2z) – 1 

​	就像logistic函数一样是S型的，连续可微的，但是值域在-1到1之间（logistic是0到1）。这会使得每层的输出或多或少的标准化，有助于加速。

Te ReLU function  

​	ReLU (z) = max (0, z)  。它是连续的但是在z=0不可微的.然而，在实践中，很实用并且可以加速计算。更重要的是，它没有最大输出值帮助梯度下降减少了一些问题。

MLP常用于分类，每次输出对应于不同的二元类（垃圾邮件/非垃圾邮件，紧急/不紧急，等等）。当类别唯一时（数字图像分类0到9），输出层通常修改为一个softmax函数。每个神经元的输出对应于相应类别的估计概率。注意到信号流只有一个方向（从输入到输出），这个架构是一个前馈神经网络（FNN）。

生物神经好像是实现一个粗糙的sigmoid阶跃函数，所以研究者很长时间限于使用sigmoid函数。但实践表明ReLU阶跃函数在ANN中更实用。



