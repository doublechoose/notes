

虽然IBM的深蓝在1996年就打败了西洋棋世界冠军Garry Kasparov ，直到最近，计算机都无法可靠的执行看似微不足道的任务，比如检测图片中的小狗或者识别说的单词。为什么这些任务对我们人类如此轻松呢？答案在于这样一个事实，即知觉主要发生在我们意识领域之外，在我们的大脑中专门的视觉，听觉和其他感觉模块内。当感官信息达到我们的意识时，它已经装饰成了高级特征;例如，当你看着一只可爱的小狗的照片时，你不能选择不看这只小狗，或不注意它的可爱。你也不能解释你如何识别一只可爱的小狗; 这对你来说很显然。因此，我们不能相信我们的主观经验：知觉不是微不足道的，为了理解它，我们必须看看感官模块是如何工作的。

卷积神经网络（CNN）是从大脑视觉皮层的研究中出现的，自20世纪80年代以来它们一直用于图像识别。在过去的几年中，由于计算能力的增加，可用训练数据的数量以及训练深网的技巧，CNN在一些复杂的视觉任务中设法实现了超人的表现。他们支持图像搜索服务，自动驾驶汽车，自动视频分类系统和更多任务。此外，CNN并不局限于视觉感知：它们在其他任务中也很成功，如语音识别或自然语言处理（NLP）; 然而，我们现在将专注于视觉应用。

在本章中，我们将介绍CNN的来源，构建块是什么样的以及如何使用TensorFlow实现它们。 然后我们将介绍一些最好的CNN架构。

## 视觉皮层架构

David H. Hubel和Torsten Wiesel在1958年和1959年（以及几年后的猴子）上对猫进行了一系列实验，对视觉皮层的结构提供了重要见解（他们因此获得了诺贝尔生理学或医学奖）。特别是，他们发现视皮层中的许多神经元有一个小的局部感受域，这意味着它们只对位于视野有限区域的视觉刺激起反应（见图13-1，其中五个局部感受域 神经元由虚线圆圈表示）。不同神经元的感受域可能重叠，并且它们一起平铺整个视野。此外，作者表明，一些神经元只对水平线的图像作出反应，而另一些神经元只对不同方向的线作出反应（两个神经元可能具有相同的感受域，但对不同的线方向作出反应）。他们还注意到一些神经元具有较大的感受域，并且它们对较复杂的模式作出反应，这些模式是较低级模式的组合。这些观察结果导致出这样的想法，即更高级别的神经元是基于相邻低级神经元的输出（在图13-1中，请注意，每个神经元只与来自前一层的少数神经元相连）。这个强大的架构能够检测视野中任何区域的各种复杂图案。
![1530520424(1).png](https://upload-images.jianshu.io/upload_images/3509189-c93c146f45f7619d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


视觉皮层的这些研究启发了1980年推出的新识别技术，后者逐渐演变为我们现在称之为卷积神经网络。一个重要的里程碑是Yann LeCun，LéonBottou，Yoshua Bengio和Patrick Haffner于1998年发表的一篇论文，该论文引入了著名的LeNet-5架构，广泛用于识别手写支票号码。 这个架构有一些你已经知道的构建块，比如完全连接的图层和sigmoid激活函数，但是它还引入了两个新的构建块：卷积层和池化层。

 为什么不简单地使用具有完全连接图层的常规深层神经网络进行图像识别任务？ 不幸的是，虽然这对于小图像（例如，MNIST）来说工作得很好，但由于它需要大量的参数，因此对于较大的图像来说它会失败。 例如，100×100图像具有10,000个像素，并且如果第一层仅具有1,000个神经元（这已经严重限制了传输到下一层的信息量），这意味着总共1000万个连接（10000像素x1000个神经元）。 而这只是第一层。 CNN使用部分连接的层来解决这个问题。

## 卷积层

CNN最重要的组成部分是卷积层：第一卷积层中的神经元不与输入图像中的每一个像素相连（就像它们在前面的章节中那样），但是仅仅与其感受域中的像素相连（参见 图13-2）。进而，第二卷积层中的每个神经元仅与位于第一层中的小矩形内的神经元连接。 这种架构允许网络专注于第一个隐藏层的低级特征，然后将它们组装成下一个隐藏层的高级特征，以此类推等等。 这种分层结构在现实世界的图像中很常见，这也是CNN在图像识别方面表现良好的原因之一。
![1530521217(1).png](https://upload-images.jianshu.io/upload_images/3509189-46c9096a2a1c2acd.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


到目前为止，我们所看到的所有多层神经网络都是由一长串神经元组成的层，我们必须在将输入图像输入到神经网络之前将其输入图像变平。 现在，每个图层都以2D表示，这使得将神经元与其相应输入进行匹配变得更加容易。

位于给定层的第i行第j列的神经元连接到位于第i行至i + fh-1，列j至j + fw-1的前一层中的神经元的输出，其中fh和fw是 感受域的高度和宽度。为了使图层具有与前一图层相同的高度和宽度，通常在输入周围添加零，如图所示。 这被称为**零填充**。
![1530521465(1).png](https://upload-images.jianshu.io/upload_images/3509189-b814a55e7f7f1563.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)



如图13-4所示，通过将感受域隔开，还可以将较大的输入层连接到更小的层。两个连续的感受域之间的距离称为步幅。在该图中，一个5×7输入层（加上零填充）连接到一个3×4层，使用3×3个感受域和一个2的步幅（在这个例子中，步幅在两个方向上都是相同的，但它并不一定如此）。位于上层的第i行第j列的神经元连接到位于行i×sh到i×sh + fh-1，列j×sw + fw-1的前一层中的神经元的输出，其中 sh和sw是垂直和水平的步幅。

![1530521738(1).png](https://upload-images.jianshu.io/upload_images/3509189-1efbfffd6a837906.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## filters

神经元的权重可以表示为感受域大小的小图像。例如，图13-5显示了两个可能的权重集，称为filters（或卷积核）.第一个被表示为中间有一条垂直白线的黑色正方形（除中央列以外，它是一个所有值为0，中央列为1的7×7矩阵）; 使用这些权重的神经元将忽略除了中心垂直线之外的感受域中的所有内容（因为除了位于中心垂直线上的所有输入都将乘以0）。第二个过滤器是一个黑色正方形，中间有一条水平白线。 再次，使用这些权重的神经元将忽略除了中心水平线之外的感受域中的一切。

现在，如果一个层里的所有神经元使用同样的垂直线滤波器（和相同的偏差），并且你你用13-5下面的图喂给网络，神经层会输出左上角的图片。注意到垂直白线会增强，而其余部分会变得模糊。同样，如果所有神经元都使用水平线滤波器，右上角图像就是你得到的; 注意水平白线得到增强，而其余部分模糊不清。因此，使用相同滤波器的充满神经元的图层将为您提供特征映射，该特征映射将突出显示图像中与滤波器最相似的区域。在训练过程中，CNN会为其任务找到最有用的过滤器，并学习将它们组合成更复杂的模式（例如，交叉是图像中垂直过滤器和水平过滤器均处于活动状态的区域）

![1530523017(1).png](https://upload-images.jianshu.io/upload_images/3509189-6f5b6aacb8a33500.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)




## 堆叠多个特征映射

到目前为止，为了简单起见，我们已经将每个卷积层表示为薄的2D层，但实际上它由几个相同大小的特征图组成，所以它在3D中更准确地表示。在一个特征映射中，所有神经元共享相同的参数（权重和偏差项），但不同的特征映射可能具有不同的参数。神经元的感受域与前面描述的相同，但是它延伸到所有先前的层的特征映射。 简而言之，卷积层同时将多个滤波器应用于其输入，使其能够检测输入中的任何位置的多个特征。

事实上，特征映射中的所有神经元共享相同的参数会显着减少模型中的参数数量，但最重要的是，这意味着一旦CNN学会识别一个位置的模式，就可以在任何其他位置识别它。 相比之下，一旦常规DNN学会识别一个位置的图案，它只能在该特定位置识别它。

而且，输入图像也由多个子图层组成：每个颜色一个通道。 通常有三种：红色，绿色和蓝色（RGB）。 灰度图像只有一个通道，但一些图像可能有更多 - 例如捕捉额外光频（如红外线）的卫星图像。

![1530524132(1).png](https://upload-images.jianshu.io/upload_images/3509189-beb5e7532688b1d5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

具体地，位于给定卷积层l中的特征映射k的行i，列j中的神经元连接到前一层1-1中的神经元的输出，位于行i×sw到i×sw + fw中。 - 1和列j×sh至j×sh + fh-1，横跨所有特征图（在层l-1中）。 请注意，位于同一行i和列j但位于不同特征映射中的所有神经元都连接到上一层中完全相同神经元的输出。


式子13-1用一个大数学等式总结了上面的解释：它给出在一个卷积层如何计算给定神经元的输出。它由于那些索引看起来很难看，但它所做的无非计算输入的权重加上偏差项。

![1530582488(1).png](https://upload-images.jianshu.io/upload_images/3509189-590503a9750868fb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


- zi,j,k是卷积层（层 l)的特征映射k里第i行，第j列的神经元输出。
- 如前所述，sh和sw是垂直和水平步幅，fh和fw是感受域的高度和宽度，fn'是前一层（层l-1）中特征映射的数量。
- xi'，j'，k'是位于层l-1，行i'，列j'，特征映射k'（或者如果前一层是输入层则为通道k'）的神经元的输出。
- bk是特征映射k的偏差项（在第1层中）。 您可以将其视为调整特征映射k的整体亮度的旋钮。
- wu，v，k'，k是层l的特征映射k中的任何神经元与位于行u，列v（相对于神经元的感受域）的输入之间的连接权重，以及特征映射k'

[下一页](https://www.jianshu.com/p/6fc2bc1d59f3)











