

为了训练RNN，技巧是通过时间展开它并简单的使用反向传播（看图14-5）。这个策略叫**通过时间反向传播（BPTT）**。

![1530683364.png](https://upload-images.jianshu.io/upload_images/3509189-576e7212b60aa60f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


就像在常规的反向传播中一样，第一次向前穿过展开的网络（由虚线箭头表示）; 然后使用成本函数C(Y(tmin),Y(tmin+1),...,Y(tmax))(当tmin和tmax是第一个和最后一个时间步输出，不计算被忽略的输出）评估输出序列，然后损失函数的梯度被反向传播经过展开的网络（由实线箭头表示）；最后，使用在BPTT期间计算的梯度更新模型参数。请注意，梯度向后流经损失函数使用的所有输出，而不仅仅是通过最终输出（例如，在图14-5中，使用网络的最后三个输出Y（2）,Y（3）和Y（4）计算成本函数，，因此梯度流过这三个输出，但不通过Y（0）和Y（1）。 此外，由于在每个时间步长使用相同的参数W和b，因此反向传播将做正确的事情并且将所有时间步长相加。

## 训练一个序列分类器

现在我们来训练一个RNN来分类MNIST图片。卷积神经网络更适合图像分类，但这只是个简单的例子，你已经十分熟悉了的。我们将每个图像视为28行28个像素的序列（因为每个MNIST图像是28×28像素）。我们将使用150个递归神经元的单元，加上一个完全连接的层，其中包含10个神经元（每个类别一个）连接到最后一个时间步的输出，然后是softmax层（见图14-6）。

![1530684107(1).png](https://upload-images.jianshu.io/upload_images/3509189-a18a623038f0cb8b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


构建期十分直接；它和我们之前构建的MNIST分类器十分相似除了用一个展开的RNN替代了隐藏层。注意到完全连接层连接到了状态张量上，它仅包含RNN的最终状态（即第28个输出）。 另请注意，y是目标类的placeholder。

```python
from tensorflow.contrib.layers import fully_connected

n_steps = 28
n_inputs = 28
n_neurons = 150
n_outputs = 18

learning_rate = 0.001

X = tf.placeholder(tf.float32,[None,n_steps,n_inputs])
y = tf.placeholder(tf.int32,[None])

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
outputs,states = tf.nn.dynamic_rnn(basic_cell,X,dtype=tf.float32)

logits = fully_connected(states,n_outputs,activation_fn=None)
xentropy = tf.nn.sparse_softmax_corss_entropy_with_logits(labels=y,logits=logits)
loss = tf.reduce_mean(xentropy)
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)
correct = tf.nn.in_top_k(logits, y, 1)
accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))
init = tf.global_variables_initializer()
```

现在我们载入MNIST数据和reshape测试数据为[batch_size,n_steps,n_inputs]。我们一会会处理reshape训练数据。

```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets("/tmp/data/")
X_test = mnist.test.images.reshape((-1, n_steps, n_inputs))
y_test = mnist.test.labels
```

现在我们可以开始训练RNN了。执行期和之前的MNIST分类器一样，除了我们对在喂给网络之前对每个训练撮进行reshape。

```python
n_epochs = 100
batch_size = 150

with tf.Session() as sess:
    init.run()
    for epoch in range(n_epochs):
        for iteration in range(mnist.train.num_examples // batch_size):
            X_batch,y_batch = mnist.train.next_batch(batch_size)
            X_batch = X_batch.reshape((-1,n_steps,n_inputs))
            sess.run(training_op,feed_dict={X:X_batch,y:y_batch})
        acc_train = accuracy.eval(feed_dict={X:X_batch,y:y_batch})
        acc_test = accuracy.eval(feed_dict={X:X_test,y:y_test})        
        print(epoch, "Train accuracy:",acc_train,"Test accuracy:",acc_test)
```

输出应该长这样：

```
0 Train accuracy: 0.713333 Test accuracy: 0.7299
1 Train accuracy: 0.766667 Test accuracy: 0.7977
...
98 Train accuracy: 0.986667 Test accuracy: 0.9777
99 Train accuracy: 0.986667 Test accuracy: 0.9809
```

我们得到了98%的准确率----不坏嘛！此外，通过调整超参数，使用He初始化初始化RNN权重，训练更长时间或添加一些正则化（例如，丢失），肯定会得到更好的结果。

可以通过将其构造代码包装在可变范围内来指定RNN的初始化程序（例如，使用variable_scope（“rnn”，initializer = variance_scaling_initializer（））来使用He初始化）

##  预测时间序列的训练

现在我们来看如何处理时间序列，如股票价格，气温，脑电波模式等等。在本节中，我们将训练RNN以预测生成的时间序列中的下一个值。每个训练实例是一个随机选择的序列，由时间序列中的20个连续值组成，目标序列与输入序列相同，除了它向前移动一个时间步（见图14-7）

![1530685482(1).png](https://upload-images.jianshu.io/upload_images/3509189-88c5878826950ce1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


首先，我们先创建RNN，它将包含100个递归神经元并且我们将用20个时间步展开它，因为每个训练实例将是20个输入长。每个输入将包含一个特征（当时的价值）。目标也是20个输入的序列，每个输入包含单个值。 代码与之前几乎相同：

```python
n_steps = 20
n_inputs = 1
n_neurons = 100
n_outputs = 1

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
```

通常，你将拥有多个输入特征。 例如，如果您尝试预测股票价格，则每个时间步骤可能会有许多其他输入特征，例如竞争股票的价格，分析师的评级或任何其他可能有助于系统进行预测的特征。

在每个时间步，我们现在有一个100大小的输出向量。但我们真正想要的是每个时间步的单个输出值。 最简单的解决方案是将单元包装在**OutputProjectionWrapper**中。单元包装器的作用类似于普通单元，代理对底层单元的每个方法调用，但它也添加了一些功能。 OutputProjectionWrapper在每个输出的顶部添加完全连接的线性神经元层（即，没有任何激活功能）（但它不影响单元状态）。 所有这些完全连接层共享相同（可训练）的权重和偏差项。 得到的RNN如图14-8所示。

![1530686026(1).png](https://upload-images.jianshu.io/upload_images/3509189-38889c0f2a6e6ee1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


包装一个单元相当简单。让我们通过将BasicRNNCell包装到OutputProjectionWrapper来调整前面的代码：

```python
cell = tf.contrib.rnn.OutputProjectionWrapper(
tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu),
output_size=n_outputs)
```

到现在为止还不错。 现在我们需要定义成本函数。 我们将使用均方误差（MSE），就像我们在之前的回归任务中所做的那样。 接下来，我们将像往常一样创建一个Adam优化器，训练操作和变量初始化操作：

```python
learning_rate = 0.001

loss = tf.reduce_mean(tf.square(outputs - y))
optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

init = tf.global_variables_initializer()
```

现在在执行期：

```python
n_iterations = 10000
batch_size = 50
with tf.Session() as sess:
    init.run()
	for iteration in range(n_iterations):
		X_batch, y_batch = [...] # fetch the next training batch
	 	sess.run(training_op, feed_dict={X: X_batch, y: y_batch})
		if iteration % 100 == 0:
			mse = loss.eval(feed_dict={X: X_batch, y: y_batch})
			print(iteration, "\tMSE:", mse)
```

输出应该长这样：

```
0 MSE: 379.586
100 MSE: 14.58426
200 MSE: 7.14066
300 MSE: 3.98528
400 MSE: 2.00254
[...]
```

模型训练好后，你就可以做预测了：

```python
X_new = [...] # New sequences
y_pred = sess.run(outputs, feed_dict={X: X_new})
```

图14-9显示了经过1,000次训练迭代后我们之前（图14-7）所见实例的预测序列

![1530686325(1).png](https://upload-images.jianshu.io/upload_images/3509189-11adb7b556549228.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


尽管使用OutputProjectionWrapper是将RNN输出序列的维度降到每个时间步长（每个实例）只有一个值的最简单的解决方案，但它并不是最有效的。有一个更机智但更有效的解决方案：你可以将[batch_size，n_steps，n_neurons]的RNN输出reshape为[batch_size * n_steps，n_neurons]，然后应用具有适当输出大小的单个完全连接层（在我们的示例中仅为1）），这将影响形状为[batch_size * n_steps，n_outputs]的输出张量，然后将此张量reshape为[batch_size，n_steps，n_outputs]。 这些操作如图14-10所示

![1530687233(1).png](https://upload-images.jianshu.io/upload_images/3509189-258be7b75f4858cb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


为了实现这个解决方案，我们首先恢复到基本单元，不用OutputProjectionWrapper：

```
cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons, activation=tf.nn.relu)
rnn_outputs, states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)
```

然后我们使用reshape（）操作堆叠所有输出，应用完全连接的线性层（不使用任何激活函数;这只是一个投影），最后使用reshape（）取消堆叠所有输出：

```
stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = fully_connected(stacked_rnn_outputs, n_outputs,
activation_fn=None)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
```

剩下的代码和之前的一样。这可以提供显着的速度提升，因为只有一个完全连接的层而不是每个时间步骤一个。

## 创意RNN

现在我们有一个可以预测未来的模型，我们可以使用它来生成一些创意序列，如本章开头所述。 我们所需要的只是为它提供一个包含n_steps值（例如，全为零）的种子序列，使用该模型预测下一个值，将该预测值附加到序列，将最后的n_steps值提供给模型以预测下一个 价值，等等。 此过程生成一个与原始时间序列有一些相似之处的新序列（参见图14-11）

```python
sequence = [0.] * n_steps
for iteration in range(300):
	X_batch = np.array(sequence[-n_steps:]).reshape(1, n_steps, 1)
	y_pred = sess.run(outputs, feed_dict={X: X_batch})
	sequence.append(y_pred[0, -1, 0])	
```

![1530687737(1).png](https://upload-images.jianshu.io/upload_images/3509189-1997cd0e1e2f4d26.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


现在你可以试着将你所有的周杰伦专辑喂给RNN，然后看它能不能生成下一个流行音乐。然而你可能需要更多强大的RNN，带有更多的神经元，并且更深。我们来看下深度RNN。

## Deep RNNs

堆叠多层单元是很常见的，如图14-12所示。 这为你提供了深度RNN。

![1530688052(1).png](https://upload-images.jianshu.io/upload_images/3509189-ffac7759c4c59898.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


为了用tf实现一个深度RNN，你可以创建多个单元并堆叠他们到一个**MultiRNNCell**中。下面代码，我们堆叠了3个独立的单元（但你可以很好地使用不同数量神经元的各种单元)：

```
n_neurons = 100
n_layers = 3

basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
multi_layer_cell = tf.contrib.rnn.MultiRNNCell([basic_cell] * n_layers)
outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
```

搞定！states变量是一个元组，每层包含一个张量，每个张量代表该层单元的最终状态（形状为[batch_size，n_neurons]）。 如果在创建MultiRNNCell时设置state_is_tuple = False，则状态变为单张量，包含来自每个层的状态，沿着列轴连接（即，其形状为[batch_size，n_layers * n_neurons]）。请注意，在TensorFlow 0.11之前， 此行为是默认行为。

## 使用多个GPU分布计算深度RNN

如果你尝试在一个不同的device()块中创建每个单元，这将不会工作：

```
with tf.device("/gpu:0"): # BAD! This is ignored.
	layer1 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
with tf.device("/gpu:1"): # BAD! Ignored again.
	layer2 = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
```

这个错误是因为BasicRNNCell是一个单元工厂，而不是一个单元；创建工厂时不会创建单元，因此也没有变量。

device 块只是简单的被忽略了。单元实际上是后面才创建的。当你调用dynamic_rnn()的时候，它调用MultiRNNCell ，这才对每个调用独立的BasicRNNCell ，这才创建出单元来（包括他们的变量）。不幸的是，这些类都没有提供任何方法来控制创建变量的设备。如果您尝试将dynamic_rnn（）调用放在设备块中，则整个RNN将固定到单个设备。 所以你被卡住了吗？ 幸运的是没有！ 诀窍是创建自己的单元格包装器：

```python
import tensorflow as tf
class DeviceCellWrapper(tf.contrib.rnn.RNNCell):
	def __init__(self, device, cell):
		self._cell = cell
		self._device = device
	@property
	def state_size(self):
		return self._cell.state_size
	@property
	def output_size(self):
		return self._cell.output_size
	def __call__(self, inputs, state, scope=None):
		with tf.device(self._device):
			return self._cell(inputs, state, scope)
```

这个包装器简单地代理每个方法调用另一个单元格，除了它将_\_call \_\_（）函数包装在一个设备块中。现在你可以在不同的GPU上分配每个层：

```
devices = ["/gpu:0", "/gpu:1", "/gpu:2"]
cells = [DeviceCellWrapper(dev,tf.contrib.rnn.BasicRNNCell(num_units=n_neurons))
for dev in devices]
multi_layer_cell = tf.contrib.rnn.MultiRNNCell(cells)
outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
```

不要设置state_is_tuple = False，否则MultiRNNCell会在单个GPU上将所有单元状态连接成单个张量。

## 应用Dropout

如果你构建一个非常深的RNN，它可能最终会过度拟合训练集。 为了防止这种情况，一种常见的技术是应用Dropout。 可以像往常一样在RNN之前或之后简单地添加一个dropout层，但如果你还想在RNN层之间应用dropout，则需要使用DropoutWrapper。 以下代码将dropout应用于RNN中每个层的输入，以50％的概率丢弃每个输入：

```
keep_prob = 0.5
cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
cell_drop = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)
multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell_drop] * n_layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
```

请注意，也可以通过设置put_keep_prob将dropout应用于输出。

这段代码的主要问题是它不仅会在训练期间而且在测试期间应用Dropout，这不是您想要的（回想一下，只有在训练期间才应用Dropout）。 不幸的是，DropoutWrapper不支持is_training placeholder（还没？），所以你必须编写自己的dropout包装类，或者有两个不同的图：一个用于训练，另一个用于测试。 第二个选项看起来像这样：

```
import sys
is_training = (sys.argv[-1] == "train")

X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.placeholder(tf.float32, [None, n_steps, n_outputs])
cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)
if is_training:
	cell = tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob=keep_prob)
multi_layer_cell = tf.contrib.rnn.MultiRNNCell([cell] * n_layers)
rnn_outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)
[...] # build the rest of the graph
init = tf.global_variables_initializer()
saver = tf.train.Saver()
with tf.Session() as sess:
	if is_training:
		init.run()
		for iteration in range(n_iterations):
			[...] # train the model
		save_path = saver.save(sess, "/tmp/my_model.ckpt")
	else:
		saver.restore(sess, "/tmp/my_model.ckpt")
		[...] # use the model
```

有了这个，你应该能够训练各种RNN！ 不幸的是，如果你想在长序列上训练RNN，事情会变得更难。 让我们看看为什么以及你可以做些什么。

## 有许多个时间步的训练的难点

为了在长序列中训练RNN，你需要用许多时间步进行训练，让展开的RNN变成一个非常深的网络。就像任何一个深神网，它也可能会收到梯度爆炸\消失的影响，并永远的训练下去。我们讨论的许多技巧也可以被用来减缓深度展开RNN的问题：好的参数初始化，非饱和激活函数（如ReLU），Batch Normalization，梯度裁剪和更快的优化器，然而，如果RNN需要处理甚至中等长度的序列（例如，100个输入），那么训练仍然会非常慢。

解决此问题的最简单和最常见的解决方案是在训练期间仅在有限数量的时间步骤内展开RNN。这称为**截断反向传播**。在tf，你可以简单的通过截断输入序列来实现它。比如，在时间序列预测问题，你只需要在训练期间减少n_steps。 当然，问题在于该模型无法学习长期模式。一种解决方法可以是确保这些缩短的序列包含旧的和最近的数据，以便模型可以学习使用两者（例如，序列可以包含过去五个月的月度数据，然后是过去五周的每周数据 ，然后是过去五天的每日数据）。 但是这种解决方法有其局限性：如果来自去年的细粒度数据实际上有用呢？ 如果有一个短暂但重要的事件，即使在几年之后，也必须考虑到这一事件（例如，选举结果），该怎么办？

除了长训练时间之外，长时间运行的RNN面临的第二个问题是第一个输入的存储器逐渐消失。 实际上，由于数据在遍历RNN时经历的变换，一些信息在每个时间步之后丢失。过了一会儿，RNN的状态几乎没有包含第一个输入的痕迹。 这可能是一个停滞不前。 例如，假设您想要对以“我喜欢这部电影”这几个词开头的长期评论进行情绪分析，但其余的评论列出了许多可能使电影更好的事情。为了解决这个问题，已经引入了具有长期记忆的各种类型的单元。 它们已经证明非常成功，基本单元不再使用了。 让我们首先看看这些长存储单元中最受欢迎的：LSTM单元。

## LSTM 单元

长短记忆（LSTM）细胞由Sepp Hochreiter和JürgenSchmidhuber在19973年提出，多年来由几位研究人员逐渐改进，例如Alex Graves，HaşimSak， Wojciech Zaremba等等。 如果你认为LSTM单元是一个黑盒子，它可以像一个基本单元一样使用，并且它会表现得更好; 训练将更快地收敛，它将检测数据中的长期依赖性。 在TensorFlow中，您只需使用BasicLSTM Cell而不是BasicRNNCell：

```
lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons)
```

LSTM单元管理两个状态向量，出于性能原因，它们默认保持独立。 创建BasicLSTMCell时,可以通过设置state_is_tuple =False更改此默认行为。

那么LSTM细胞如何工作？ 基本LSTM单元的架构如图14-13所示。

![1530691134(1).png](https://upload-images.jianshu.io/upload_images/3509189-6734734d02fb42b6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


如果您不查看框内的内容，LSTM单元看起来与常规单元完全相同，除了它的状态分为两个向量：h（t）和c（t）（“c”代表“cell”）。 您可以将h（t）视为短期状态，将c（t）视为长期状态。

现在让我们打开盒子吧！ 关键的想法是，网络可以了解在长期状态下存储什么，丢弃什么以及从中读取什么。

当长期状态c（t-1）从左到右穿过网络时，你可以看到它首先通过一个遗忘门，丢弃了一些记忆，然后通过加法操作增加了一些新记忆（这增加了 由输入门选择的记忆。 结果c（t）直接发送，没有任何进一步的转换。 因此，在每个时间步，一些记忆被删除，一些记忆被添加。 此外，在加法运算之后，长期状态被复制并通过tanh函数，然后结果被输出门过滤。 这产生短期状态h（t）（其等于该时间步长y（t）的单元输出）。 现在让我们来看看新记忆的来源以及大门的工作方式。

首先，将当前输入矢量x（t）和先前的短期状态h（t-1）馈送到四个不同的完全连接的层。 它们都有不同的用途：

- 主层是输出g（t）的层。 它具有分析当前输入x（t）和先前（短期）状态h（t-1）的通常作用。 在基本单元格中，除了这一层之外别无其他，它的输出直接输出到y（t）和h（t）。 相反，在LSTM单元格中，此图层的输出不会直接输出，而是部分存储在长期状态中。
- 其他三个层是门控制器。 由于它们使用逻辑激活功能，它们的输出范围从0到1.正如您所看到的，它们的输出被馈送到元素乘法运算，因此如果它们输出0，它们关闭门，如果它们输出1，则它们打开 它。 特别的：
  - 遗忘门（由f（t）控制）控制长期状态的哪些部分应该被擦除。
  - 输入门（由i（t）控制）控制g（t）的哪些部分应加到长期状态（这就是为什么我们说它只是“部分存储”）。
  - 最后，输出门（由o（t）控制）控制长时间状态的哪个部分应该在此时间步读取（输出到h（t））和y（t）

简而言之，LSTM单元可以学习识别重要的输入（这是输入门的作用），将其存储在长期状态，学会在需要时保留它（这就是遗忘门的作用） ），并学习在需要时提取它。 这就解释了为什么他们在捕捉时间序列，长文本，录音等长期模式方面取得了惊人的成功。

公式14-3总结了如何计算单个实例的每个时间步长的单元的长期状态，短期状态和输出（整个小批量的方程非常相似）

![1530691692(1).png](https://upload-images.jianshu.io/upload_images/3509189-8adadf09fd3ea17e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


## Peephole Connections 

在基本LSTM单元中，门控制器只能看输入x（t）和前一个短期状态h（t-1）。 通过让他们看一下长期状态来给他们更多的上下文可能是一个好主意。 这个想法是由Felix Gers和JürgenSchmidhuber在2000年提出的。他们提出了一种带有额外连接的LSTM变体，称为窥孔连接：将前一个长期状态c（t-1）作为输入添加到忘记门的控制器和 输入门和当前长期项c（t）作为输入添加到输出门的控制器。

要在TensorFlow中实现窥孔连接，必须使用LSTMCell而不是BasicLSTMCell并设置use_peepholes = True：

```
lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True)
```

LSTM细胞还有许多其他变种。 一个特别受欢迎的变体是GRU单元，我们现在将会看到它。

## GRU 单元

门控递归单元（GRU）细胞（见图14-14）由Kyunghyun Cho等人提出。 在2014年的论文中，还介绍了我们前面提到的编码器 - 解码器网络。

![1530692001(1).png](https://upload-images.jianshu.io/upload_images/3509189-e27657cda0c13f9f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


GRU单元是LSTM单元的简化版本，它似乎也表现良好（这解释了它越来越受欢迎）。 主要简化是：

- 两个状态向量合并为单个向量h（t）
- 单个门控制器控制忘记门和输入门。 如果门控制器输出1，则输入门打开，忘记门关闭。如果它输出一个0，相反的情况发生。 换句话说，每当必须存储记忆时，首先擦除存储它的位置。 这实际上是LSTM细胞本身的常见变体。
- 没有输出门; 每个时间步输出满状态向量。 但是，有一个新的门控制器控制先前状态的哪个部分将显示到主层。

公式14-4总结了如何计算单个实例的每个时间步的单元状态。

![1530692172(1).png](https://upload-images.jianshu.io/upload_images/3509189-1773f10a9bb36531.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


在TensorFlow中创建GRU单元是容易的：

```
gru_cell = tf.contrib.rnn.GRUCell(num_units=n_neurons)
```

LSTM或GRU单元是近年来RNN成功背后的主要原因之一，特别是在自然语言处理（NLP）中的应用。

## 自然语言处理

大多数最先进的NLP应用程序，例如机器翻译，自动摘要，解析，情感分析等，现在（至少部分地）基于RNN。 在最后一节中，我们将快速了解机器翻译模型的外观。 TensorFlow强大的[Word2Vec](https://www.tensorflow.org/tutorials/word2vec)和[Seq2Seq](https://www.tensorflow.org/tutorials/seq2seq)教程很好地介绍了这个主题。

### 文字嵌入

在开始之前，我们需要选择一个单词表示。一种选择可以是使用one-hot矢量来表示每个单词。假设您的词汇表包含50,000个单词，那么第n个单词将被表示为50,000维向量，除了第n个位置的1之外，其全部为0。然而，如此庞大的词汇量，这种稀疏表示根本就没有效率。 理想情况下，您希望类似的单词具有相似的表示形式，使模型可以轻松地将所学单词的内容概括为所有相似的单词。例如，如果模型被告知“I drink milk ”是一个有效的句子，并且如果它知道“milk ”接近“water ”但远离“shoes ”，那么它会知道“I drink water ”也可能是一个有效的句子，而“I drink shoes ”可能不是。 但是你怎么能想出这样一个有意义的代表呢？

最常见的解决方案是使用相当小且密集的矢量（例如，150维度）来表示词汇表中的每个单词，称为**嵌入**，并且让神经网络在训练期间学习每个单词的良好嵌入。在训练开始时，嵌入只是随机选择，但在训练期间，反向传播会自动移动嵌入，以帮助神经网络执行其任务。通常这意味着相似的词会逐渐彼此靠近，甚至最终以一种相当有意义的方式组织起来。 例如，嵌入可能最终沿着代表性别，单数/复数，形容词/名词等的各种轴放置。 结果真的很棒。

在TensorFlow中，首先需要创建表示词汇表中每个单词的嵌入的变量（随机初始化）：

```python
vocabulary_size = 50000
embedding_size = 150
embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))
```

现在假设您想要将“I drink milk”这句话提供给您的神经网络。 您应首先预处理该句子并将其分解为已知单词列表。 例如，您可以删除不必要的字符，用预定义的标记字替换未知字，例如“[UNK]”，用“[NUM]”替换数值，用“[URL]”替换URL，依此类推。获得已知单词列表后，您可以在字典中查找每个单词的整数标识符（从0到49999），例如[72,3335,288]。 此时，您已准备好使用占位符将这些单词标识符提供给TensorFlow，并应用embedding_lookup（）函数来获取相应的嵌入：

```
train_inputs = tf.placeholder(tf.int32, shape=[None]) # from ids...
embed = tf.nn.embedding_lookup(embeddings, train_inputs) # ...to embeddings
```

一旦你的模型学会了很好的单词嵌入，它们实际上可以在任何NLP应用程序中相当有效地重复使用：毕竟，无论你的应用是什么，“milk ”仍然接近“water ”而远离“shoes ”。 实际上，您可能希望下载预训练的单词嵌入，而不是训练自己的单词嵌入。 就像重新使用预训练图层一样（参见第11章），您可以选择冻结预训练嵌入（例如，使用trainable = False创建嵌入变量）或让反向传播为您的应用程序调整它们。 第一个选项将加快培训速度，但第二个选项可能会导致性能稍高。

嵌入对于表示可以采用大量不同值的分类属性也很有用，尤其是当值之间存在复杂的相似性时。 例如，考虑职业，爱好，菜肴，品种，品牌等。

您现在拥有了实现机器翻译系统所需的几乎所有工具。 

## 机器翻译的编码器-解码器网络

让我们来看一个[简单的机器翻译模型](https://arxiv.org/pdf/1409.3215v3.pdf)，它将英语句子翻译成法语（见图14-15）

![1530693150(1).png](https://upload-images.jianshu.io/upload_images/3509189-e6a8ad43e06c9e17.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

英语句子被送到编码器，解码器输出法语翻译。 请注意，法语翻译也用作解码器的输入，但后退一步。 换句话说，解码器作为输入给出它应该在前一步骤输出的字（不管它实际输出的是什么）。对于第一个单词，给出一个表示句子开头的标记（例如，“<go>”）。 期望解码器以句末结束（EOS）令牌（例如，“<eos>”）结束句子。

每个单词最初由简单的整数标识符表示（例如，单词“milk”为288）。 接下来，嵌入查找返回单词嵌入（如前所述，这是一个密集的，相当低维度的向量）。 这些字嵌入是实际送到编码器和解码器的内容.

在每个步骤，解码器输出输出词汇表中每个单词的分数（即法语），然后Softmax层将这些分数变成概率。 例如，在第一步，单词“Je”可能具有20％的概率，“Tu”可能具有1％的概率，等等。 输出概率最高的单词。 这非常类似于常规分类任务，因此您可以用softmax_cross_entropy_with_logits（）函数训练模型。

请注意，在推理时间（训练后），您将无法将目标句子提供给解码器。 相反，只需向解码器提供上一步输出的字，如图14-16所示（这将需要一个未在图中显示的嵌入查找）。

![1530693437(1).png](https://upload-images.jianshu.io/upload_images/3509189-c2c6a9d225cae20e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


好的，现在你有了整体的认识。 但是，如果您通过TensorFlow的序列到序列教程并查看rnn / translate / seq2seq_model.py中的代码（在[TensorFlow模型中](https://github.com/tensorflow/models)），您会发现一些重要的差异：

- 首先，到目前为止，我们假设所有输入序列（到编码器和到解码器）具有恒定的长度。但显然句子长度可能不同。有几种方法可以处理 - 例如，使用static_rnn（）或dynamic_rnn（）方法的sequence_length参数来指定每个句子的长度（如前所述）。然而，本教程中使用了另一种方法（大概是出于性能原因）：句子被分组成相似长度的桶（例如，1到6个单词的句子，7到12个单词的句子，依此类推），并使用特殊的填充标记（例如，“<pad>”）填充较短的句子。例如“I drink
  milk”变成“<pad> <pad> <pad> milk drink I”，其翻译成为“Je bois du lait <eos> <pad>”。当然，我们想要忽略经过EOS token的任何输出。为此，教程的实现使用了target_weights向量。例如，对于目标句子“Je bois du lait<eos> <pad>“，权重将设置为[1.0,1.0,1.0,1.0,1.0,0.0]（注意与目标句子中的填充标记对应的权重0.0）。简单地将损失乘以目标权重将使与EOS token之后的单词相对应的损失归零。
- 其次，当输出词汇量很大时（这里就是这种情况），输出每个可能单词的概率将非常慢。 如果目标词汇表包含例如50,000个法语单词，则解码器将输出50,000维向量，然后在这样大的向量上计算softmax函数将是非常计算密集的。 为了避免这种情况，一种解决方案是让解码器输出更小的矢量，例如1,000维矢量，然后使用采样技术来估计损失，而不必在目标词汇表中的每个单词上计算它。 这个Sampled Softmax技术是由SébastienJean等人于2015年推出的。在TensorFlow中，可以使用sampled_softmax_loss（）函数。
- 第三，教程的实现使用一种注意机制，让解码器窥视输入序列。 增加注意力的RNN超出了本书的范围，但如果您感兴趣，可以使用注意力提供有关机器翻译，机器阅读和图像标题的有用文章。
- 最后，本教程的实现使用了tf.nn.legacy_seq2seq模块，该模块提供了轻松构建各种编码器 - 解码器模型的工具。

例如，embedding_rnn_seq2seq（）函数创建一个简单的编码器 - 解码器模型，自动为您处理字嵌入，就像图14-15中所示的那样。 此代码可能会快速更新以使用新的tf.nn.seq2seq模块。

您现在拥有了解序列到序列教程实现所需的所有工具。 试着训练自己的英语到法语的翻译器吧！
